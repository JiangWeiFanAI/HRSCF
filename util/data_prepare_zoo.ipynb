{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c36a8f474e1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# last=[]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./train/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mlast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"123\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_files\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_files' is not defined"
     ]
    }
   ],
   "source": [
    "#有递归文件夹（iris）\n",
    "def list_all_files(rootdir):\n",
    "    import os\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i])\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(list_all_files(path))\n",
    "        if os.path.isfile(path):\n",
    "            _files.append(path)\n",
    "    return _files\n",
    "\n",
    "\n",
    "\n",
    "dataset_train=[]\n",
    "dataset_test=[]\n",
    "count_train=0\n",
    "count_valid=0\n",
    "flag=0\n",
    "# last=[]\n",
    "print(\"./train/\"+\"/\".join(all_files[0].split(\"/\")[7:]))\n",
    "last=\"123\"\n",
    "for file in all_files:\n",
    "#     print(1)\n",
    "    now=str(file.split(\"/\")[-3])+str(file.split(\"/\")[-2])\n",
    "    if now !=last and flag<=1:\n",
    "        dataset_test.append(file)\n",
    "        \n",
    "        flag+=1\n",
    "#         print(last)\n",
    "    else:\n",
    "        last=str(file.split(\"/\")[-3])+str(file.split(\"/\")[-2])\n",
    "        dataset_train.append(file)\n",
    "        flag=0\n",
    "\n",
    "print(len(dataset_test))\n",
    "print(len(dataset_train))\n",
    "\n",
    "\n",
    "#分成train和val\n",
    "root=\"/home/zheng/jxx/jxx_iris/my_iris/val/\"\n",
    "for i in dataset_test:\n",
    "#     print(root+\"/\".join(i.split(\"/\")[7:8]))\n",
    "#     print(i)\n",
    "    dst_path=root+\"/\".join(i.split(\"/\")[7:8])\n",
    "    if not os.path.exists(dst_path):\n",
    "        os.mkdir(dst_path)\n",
    "        \n",
    "    dst_path=root+\"/\".join(i.split(\"/\")[7:9])  \n",
    "    if not os.path.exists(dst_path):\n",
    "        os.mkdir(dst_path)\n",
    "        \n",
    "    dst_path=root+\"/\".join(i.split(\"/\")[7:10])\n",
    "    copyfile(i, dst_path)  \n",
    "    \n",
    "root=\"/home/zheng/jxx/jxx_iris/my_iris/train/\"\n",
    "for i in dataset_train:\n",
    "#     print(root+\"/\".join(i.split(\"/\")[7:8]))\n",
    "#     print(i)\n",
    "    dst_path=root+\"/\".join(i.split(\"/\")[7:8])\n",
    "    if not os.path.exists(dst_path):\n",
    "        os.mkdir(dst_path)\n",
    "        \n",
    "    dst_path=root+\"/\".join(i.split(\"/\")[7:9])  \n",
    "    if not os.path.exists(dst_path):\n",
    "        os.mkdir(dst_path)\n",
    "        \n",
    "    dst_path=root+\"/\".join(i.split(\"/\")[7:10])\n",
    "    copyfile(i, dst_path) \n",
    "\n",
    "\n",
    "class CASIA_iris(object):\n",
    "    def __init__(self, root='/home/zheng/data', **kwargs):\n",
    "        self.root_dir=root\n",
    "#         self._check_before_run()\n",
    "        self.train_file=self._list_all_files(\"./train\")\n",
    "        self.val_file=self._list_all_files(\"./val\")\n",
    "#         print(len(self.val_file))\n",
    "        self.train,self.valid = self._process_dir()\n",
    "    def _check_before_run(self):\n",
    "        if not osp.exists(self.data_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.data_dir))\n",
    "    \n",
    "    def _process_dir(self, relabel=False):\n",
    "        train_data=[]\n",
    "        val_data=[]\n",
    "        for i in self.train_file:\n",
    "            train_data.append((i,i.split(\"/\")[2]))\n",
    "           \n",
    "        for i in self.val_file:\n",
    "            val_data.append((i,i.split(\"/\")[2]))\n",
    "        \n",
    "        \n",
    "        print(\"the number of train set:{}\".format(len(train_data)))\n",
    "        print(\"the number of validation set:{}\".format(len(val_data)))\n",
    "        return train_data,val_data\n",
    "\n",
    "    def _list_all_files(self,root_dir):\n",
    "        _files = []\n",
    "        list = os.listdir(root_dir) #列出文件夹下所有的目录与文件\n",
    "        for i in range(0,len(list)):\n",
    "            path = os.path.join(root_dir,list[i])\n",
    "            if os.path.isdir(path):\n",
    "                _files.extend(self._list_all_files(path))\n",
    "            if os.path.isfile(path):\n",
    "                _files.append(path)\n",
    "        return _files\n",
    "\n",
    "dataset=CASIA_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../save/result/my_217'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path='../save/result/my_217'\n",
    "\n",
    "def mkdir(path):\n",
    "    import os\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "create_dir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取文件夹下所有图片（没有递归文件夹）\n",
    "# train_ds = gdata.vision.ImageFolderDataset(\n",
    "#     os.path.join(data_dir, input_dir, 'train'), flag=1)\n",
    "\n",
    "train_imgs = []\n",
    "train_race= []\n",
    "train_label=[]\n",
    "train_age=[]\n",
    "train_gender=[]\n",
    "\n",
    "val_imgs = []\n",
    "val_race= []\n",
    "val_age=[]\n",
    "val_gender=[]\n",
    "val_label = []\n",
    "\n",
    "\n",
    "file_name=glob(\"./UTKFace/*jpg\")\n",
    "for i in file_name:\n",
    "    \n",
    "    try:#清洗数据集\n",
    "        if int(i.split(\"/\")[2].split(\"_\")[2])==0:\n",
    "            if random.random()>0.33:\n",
    "                continue\n",
    "    except:\n",
    "        print(i)\n",
    "        continue\n",
    "            \n",
    "            \n",
    "    if random.random()>0.8:\n",
    "        try:#清洗数据集\n",
    "            double=[]\n",
    "            \n",
    "#             val_race.append(int(i.split(\"/\")[2].split(\"_\")[2]))\n",
    "#             val_gender.append(i.split(\"/\")[2].split(\"_\")[1])\n",
    "            double.append(int(i.split(\"/\")[2].split(\"_\")[2]))\n",
    "            double.append(int(i.split(\"/\")[2].split(\"_\")[1]))\n",
    "#             print (double)\n",
    "            val_label.append(double)\n",
    "        except:\n",
    "            print(i)\n",
    "            continue\n",
    "        if int(i.split(\"/\")[2].split(\"_\")[2])>4:\n",
    "            print (i)\n",
    "            continue\n",
    "        img=cv2.imread(i)\n",
    "#         img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        img=cv2.resize(img,(image_size,image_size))\n",
    "        val_imgs.append(list(np.array(img).reshape(image_size,image_size,3)))\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "#             train_race.append(int(i.split(\"/\")[2].split(\"_\")[2]))\n",
    "            double=[]\n",
    "\n",
    "            double.append(int(i.split(\"/\")[2].split(\"_\")[2]))\n",
    "            double.append(int(i.split(\"/\")[2].split(\"_\")[1]))\n",
    "            train_label.append(double)\n",
    "        except:\n",
    "            print(i)\n",
    "            continue\n",
    "        if int(i.split(\"/\")[2].split(\"_\")[2])>4:\n",
    "            print (i)\n",
    "            continue\n",
    "        img=cv2.imread(i)\n",
    "#         import mxnet.image as mximg\n",
    "# img=mximg.imread(query_img_path[5])\n",
    "#         img=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        img=cv2.resize(img,(image_size,image_size))\n",
    "        \n",
    "        train_imgs.append(list(np.array(img).reshape(image_size,image_size,3)))\n",
    "\n",
    "\n",
    "# def load_image(image_path, image_size):\n",
    "#     file_name=glob(image_path+\"/*jpg\")\n",
    "#     sample = []\n",
    "#     for file in file_name:\n",
    "#         pic = imread(file).astype(np.float32)\n",
    "#         pic = imresize(pic, (image_size, image_size)).astype(np.float32)\n",
    "#         sample.append(pic)\n",
    " \n",
    "#     sample = np.array(sample)\n",
    "#     return sample\n",
    "\n",
    "\n",
    "# file_name=glob(\"./UTKFace/*jpg\")#更新，随机数种子制作验证集\n",
    "# count =0\n",
    "# for i in file_name:\n",
    "# #     if count ==1000:\n",
    "# #         break\n",
    "#     age=i.split(\"/\")[2].split(\"_\")[0]\n",
    "# #     if age>'100':\n",
    "# #         train_age.append(100)\n",
    "# #     else:\n",
    "# #         train_age.append(i.split(\"/\")[2].split(\"_\")[0])\n",
    "# #     train_gender.append(i.split(\"/\")[2].split(\"_\")[1])\n",
    "#     try:\n",
    "#         train_race.append(int(i.split(\"/\")[2].split(\"_\")[2]))\n",
    "#     except:\n",
    "#         print(i)\n",
    "#         continue\n",
    "#     if int(i.split(\"/\")[2].split(\"_\")[2])>4:\n",
    "#         print (i)\n",
    "#         continue\n",
    "#     img=cv2.imread(i)\n",
    "#     img=cv2.resize(img,(image_size,image_size))\n",
    "#     train_imgs.append(img)\n",
    "#     count+=1\n",
    "    \n",
    "# #     print (i.split(\"/\")[2].split(\"_\")[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将松散的image 分好类(copyfile 的使用)\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "from shutil import copyfile,move\n",
    "\n",
    "root=\"/home/zheng/jxx/jxx_iris/my_iris/iris-1000/iris_class/iris_clean/\"\n",
    "\n",
    "file_name=glob(\"./iris_clean/*jpg\")\n",
    "for i in file_name:\n",
    "#     print(i.split(\"/\")[-1].split(\".\"))\n",
    "    label=i.split(\"/\")[-1].split(\".\")[0][2:5]\n",
    "    deraction=i.split(\"/\")[-1].split(\".\")[0][5]\n",
    "#     print(label,deraction)\n",
    "    \n",
    "    dst_path=root+label+\"/\"\n",
    "    if not os.path.exists(dst_path):\n",
    "        os.mkdir(dst_path)\n",
    "        \n",
    "    dst_path=dst_path+deraction+\"/\"\n",
    "    \n",
    "    if not os.path.exists(dst_path):\n",
    "        os.mkdir(dst_path)\n",
    "#     print(label)\n",
    "    dst_path=dst_path+i.split(\"/\")[-1]\n",
    "#     print(dst_path)\n",
    "    \n",
    "    copyfile(i, dst_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存所有features到csv中,慢的ya批\n",
    "\n",
    "# trans=TestTransform(224,224)\n",
    "\n",
    "count=0\n",
    "start=time.time()\n",
    "with open(\"./a.csv\",\"a\") as output:\n",
    "    for i in train_file:\n",
    "    #     if count%100==0:\n",
    "    #         print(count,time.time()-start)\n",
    "    #         start=time.time()\n",
    "        img=imread(i)\n",
    "        img=trans(img).reshape(1,3,224,224)\n",
    "        img = img.as_in_context(mx.gpu(0))\n",
    "        feature=net.features(img)\n",
    "        feature=nd.squeeze(feature).asnumpy().tolist()\n",
    "        label= i.split(\"/\")[2]\n",
    "        feature=[str(i) for i in feature]\n",
    "        feature=\" \".join(feature)#string\n",
    "        output.write(feature+\",\"+str(label)+\"\\n\")\n",
    "#         count+=1\n",
    "        if count==10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存所有features到csv中 ver gpu turbo\n",
    "\n",
    "ctx=mx.gpu(0)\n",
    "with open(\"./features_shuffled.csv\",\"a\") as output:#接着写\n",
    "\n",
    "    for i, (data, label) in enumerate(trainloader):#注意dataloder\n",
    "        start = time.time()\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        feature=net.features(data)\n",
    "        for index,feat in enumerate(feature):\n",
    "            feat=nd.squeeze(feat).asnumpy().tolist()\n",
    "            tag= label[index].asscalar()\n",
    "            feat=[str(i) for i in feat]\n",
    "            feat=\" \".join(feat)#string\n",
    "    #         print(feat)\n",
    "#             print(tag)\n",
    "            output.write(feat+\",\"+str(tag)+\"\\n\")\n",
    "        print(time.time()-start)\n",
    "        \n",
    "        \n",
    "fp=open(path,\"r\")\n",
    "reader= csv.reader(fp)\n",
    "# data\n",
    "\n",
    "\n",
    "count=0\n",
    "for i in reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存rec文件\n",
    "saveprefix_rec = './train_224_multi' + '.rec'\n",
    "saveprefix_idx = './train_224_multi' + '.idx'\n",
    "record = mx.recordio.MXIndexedRecordIO(saveprefix_idx,\n",
    "                                       saveprefix_rec, 'w')\n",
    "id_x=0\n",
    "ses=[]\n",
    "for img,label in zip(train_imgs,train_label):\n",
    "    if id_x==10000:\n",
    "        plt.imshow(np.array(train_imgs[id_x]))\n",
    "        print (train_label[id_x])\n",
    "    race = label[0]\n",
    "    gender = label[1]\n",
    "    header = mx.recordio.IRHeader(flag=0,label=(race,gender),id=0,id2=0)\n",
    "    s = mx.recordio.pack_img(header, np.array(img))\n",
    "    ses.append(s)\n",
    "#     ses.shuffle()\n",
    "    record.write_idx(id_x,s)\n",
    "    id_x+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取rec文件\n",
    "imgrec=recordio.MXIndexedRecordIO(\"./train_asian_224.idx\",\"./train_asian_224.rec\",'r')\n",
    "# train_imgs=[]\n",
    "# train_race =[]\n",
    "# val_imgs = []\n",
    "# val_race= []\n",
    "# val_ages = []\n",
    "\n",
    "\n",
    "for _ in range(3500):\n",
    "    s=imgrec.read()\n",
    "    header,img=recordio.unpack(s)\n",
    "    train_race.append(2)\n",
    "    a= list(mx.image.imdecode(img).asnumpy())\n",
    "    train_imgs.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import sys\n",
    "from os import path as osp\n",
    "from pprint import pprint\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxboard import SummaryWriter\n",
    "\n",
    "from mxnet.gluon.data import DataLoader\n",
    "from mxnet.gluon import data as gdata,loss as gloss,model_zoo\n",
    "\n",
    "import os\n",
    "import gluonbook as gb\n",
    "from mxnet import autograd, gluon, image, init, nd\n",
    "from mxnet.gluon import data as gdata, loss as gloss, utils as gutils\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import logging\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "import mxnet.optimizer as optimizer\n",
    "import sklearn\n",
    "import scipy.io as sio\n",
    "\n",
    "from glob import glob\n",
    "from mxnet import recordio\n",
    "import string\n",
    "from PIL import Image\n",
    "import time\n",
    "import cv2\n",
    "\n",
    "import scipy.io as scio\n",
    "\n",
    "# import argparse\n",
    "# import scipy.io\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from torchvision import datasets\n",
    "# import matplotlib\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from config import opt\n",
    "# from datasets import data_manager\n",
    "from datasets.data_provider import ImageData\n",
    "from datasets.samplers import RandomIdentitySampler\n",
    "from models import get_baseline_model\n",
    "from trainers import reidEvaluator, reidTrainer\n",
    "from utils.loss import TripletLoss\n",
    "from utils.serialization import Logger\n",
    "from utils.serialization import save_checkpoint\n",
    "from utils.transforms import TrainTransform, TestTransform\n",
    "\n",
    "seed = 0\n",
    "ctx=mx.gpu(0)\n",
    "# dataset options\n",
    "dataset = 'market'\n",
    "height = 256\n",
    "width = 128\n",
    "\n",
    "# optimization options\n",
    "optim = 'adam'\n",
    "max_epoch = 300\n",
    "train_batch = 128\n",
    "test_batch = 128\n",
    "lr = 3e-4\n",
    "step_size = 40\n",
    "gamma = 0.1\n",
    "weight_decay = 5e-4\n",
    "momentum = 0.9\n",
    "margin = 0.3\n",
    "num_instances = 4\n",
    "num_gpu = 1\n",
    "\n",
    "# model options\n",
    "model_name = 'softmax_triplet'  # softmax, triplet, softmax_triplet\n",
    "last_stride = 1\n",
    "pretrained_model = '/home/zheng/.mxnet/models/resnet50_v1-0aee57f9.params'\n",
    "load_model = '/home/zheng/mxnet-ckpt/'\n",
    "\n",
    "# miscs\n",
    "print_freq = 30\n",
    "eval_step = 50\n",
    "save_dir = \"/home/zheng/jxx/jxx_reid/reid_baseline_gluon-master/train_models\"\n",
    "workers = 8\n",
    "start_epoch = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import glob\n",
    "import re\n",
    "from os import path as osp\n",
    "\n",
    "\"\"\"Dataset classes\"\"\"\n",
    "\n",
    "\n",
    "class Market1501(object):\n",
    "\n",
    "    dataset_dir = 'Market-1501-v15.09.15'\n",
    "\n",
    "    def __init__(self, root='/home/zheng/data', **kwargs):\n",
    "        self.dataset_dir = osp.join(root, self.dataset_dir)\n",
    "        self.train_dir = osp.join(self.dataset_dir, 'bounding_box_train')\n",
    "        self.query_dir = osp.join(self.dataset_dir, 'query')\n",
    "        self.gallery_dir = osp.join(self.dataset_dir, 'bounding_box_test')\n",
    "\n",
    "        self._check_before_run()\n",
    "\n",
    "        train, num_train_pids, num_train_imgs = self._process_dir(self.train_dir, relabel=True)\n",
    "        query, num_query_pids, num_query_imgs = self._process_dir(self.query_dir, relabel=False)\n",
    "        gallery, num_gallery_pids, num_gallery_imgs = self._process_dir(self.gallery_dir, relabel=False)\n",
    "        num_total_pids = num_train_pids + num_query_pids\n",
    "        num_total_imgs = num_train_imgs + num_query_imgs + num_gallery_imgs\n",
    "\n",
    "        print(\"=> Market1501 loaded\")\n",
    "        print(\"Dataset statistics:\")\n",
    "        print(\"  ------------------------------\")\n",
    "        print(\"  subset   | # ids | # images\")\n",
    "        print(\"  ------------------------------\")\n",
    "        print(\"  train    | {:5d} | {:8d}\".format(num_train_pids, num_train_imgs))\n",
    "        print(\"  query    | {:5d} | {:8d}\".format(num_query_pids, num_query_imgs))\n",
    "        print(\"  gallery  | {:5d} | {:8d}\".format(num_gallery_pids, num_gallery_imgs))\n",
    "        print(\"  ------------------------------\")\n",
    "        print(\"  total    | {:5d} | {:8d}\".format(num_total_pids, num_total_imgs))\n",
    "        print(\"  ------------------------------\")\n",
    "\n",
    "        self.train = train\n",
    "        self.query = query\n",
    "        self.gallery = gallery\n",
    "\n",
    "        self.num_train_pids = num_train_pids\n",
    "        self.num_query_pids = num_query_pids\n",
    "        self.num_gallery_pids = num_gallery_pids\n",
    "\n",
    "    def _check_before_run(self):\n",
    "        \"\"\"Check if all files are available before going deeper\"\"\"\n",
    "        if not osp.exists(self.dataset_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.dataset_dir))\n",
    "        if not osp.exists(self.train_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.train_dir))\n",
    "        if not osp.exists(self.query_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.query_dir))\n",
    "        if not osp.exists(self.gallery_dir):\n",
    "            raise RuntimeError(\"'{}' is not available\".format(self.gallery_dir))\n",
    "\n",
    "    def _process_dir(self, dir_path, relabel=False):\n",
    "        img_paths = glob.glob(osp.join(dir_path, '*.jpg'))\n",
    "        pattern = re.compile(r'([-\\d]+)_c(\\d)')\n",
    "\n",
    "        pid_container = set()\n",
    "        for img_path in img_paths:\n",
    "            pid, _ = map(int, pattern.search(img_path).groups())\n",
    "            if pid == -1: continue  # junk images are just ignored\n",
    "            pid_container.add(pid)\n",
    "        pid2label = {pid: label for label, pid in enumerate(pid_container)}\n",
    "\n",
    "        dataset = []\n",
    "        for img_path in img_paths:\n",
    "            pid, camid = map(int, pattern.search(img_path).groups())\n",
    "            if pid == -1:\n",
    "                continue  # junk images are just ignored\n",
    "            assert 0 <= pid <= 1501  # pid == 0 means background\n",
    "            assert 1 <= camid <= 6\n",
    "            camid -= 1  # index starts from 0\n",
    "            if relabel: pid = pid2label[pid]\n",
    "            dataset.append((img_path, pid, camid))\n",
    "\n",
    "        num_pids = len(pid_container)\n",
    "        num_imgs = len(dataset)\n",
    "        return dataset, num_pids, num_imgs\n",
    "\n",
    "\n",
    "\n",
    "__factory = {\n",
    "    'market': Market1501\n",
    "}\n",
    "\n",
    "\n",
    "class RandomIdentitySampler(Sampler):\n",
    "    def __init__(self, data_source, num_instances=4):\n",
    "        self.data_source = data_source#train_img\n",
    "        self.num_instances = num_instances\n",
    "        self.index_dic = defaultdict(list)\n",
    "        for index, (_, pid, _) in enumerate(data_source):\n",
    "            self.index_dic[pid].append(index)#第几个是当前类别的\n",
    "        self.pids = list(self.index_dic.keys())#类别编号\n",
    "        self.num_identities = len(self.pids)#类别总数\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = np.random.permutation(self.num_identities)\n",
    "        ret = []\n",
    "        for i in indices:\n",
    "            pid = self.pids[i]\n",
    "            t = self.index_dic[pid]\n",
    "            replace = False if len(t) >= self.num_instances else True\n",
    "            t = np.random.choice(t, size=self.num_instances, replace=replace)\n",
    "            ret.extend(t)\n",
    "        return iter(ret)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_identities * self.num_instances#751*4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.mat 文件写入 读取\n",
    "import scipy.io as scio\n",
    "\n",
    "\n",
    "result = {'gallery_f':gallery_feature.asnumpy(),\n",
    "          'gallery_label':gallery_label.asnumpy(),\n",
    "          'gallery_cam':gallery_cam.asnumpy(),\n",
    "          'query_f':query_feature.asnumpy(),\n",
    "          'query_label':query_label.asnumpy(),\n",
    "          'query_cam':query_cam.asnumpy()\n",
    "         }\n",
    "# result={\"label\":gallery_label.asnumpy()}\n",
    "sio.savemat('my_result_dens161.mat',result)\n",
    "\n",
    "\n",
    "load_data = sio.loadmat('my_result_dens161.mat')\n",
    "query_feature =nd.array(load_data['query_f'],ctx=ctx)\n",
    "query_cam = load_data['query_cam'][0]\n",
    "# query_label = nd.array([result['query_label'][0]],ctx=ctx)\n",
    "query_label = load_data['query_label'][0]\n",
    "# print (query_cam)\n",
    "gallery_feature = nd.array(load_data['gallery_f'],ctx=ctx)\n",
    "gallery_cam = load_data['gallery_cam'][0]\n",
    "gallery_label = load_data['gallery_label'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdata.vison.ImageFolderDataset(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.concat#矩阵拼接\n",
    "query_feature=nd.concat(query_feature,t_feature,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet.gluon.data.vision.transforms as T\n",
    "class TestTransform(object):\n",
    "    def __init__(self, h, w):\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "\n",
    "    def __call__(self, x=None):\n",
    "        x = T.Resize((self.w, self.h))(x)\n",
    "#         x = T.ToTensor()(x)\n",
    "#         x = T.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "#                         std=(0.229, 0.224, 0.225))(x)\n",
    "        return x\n",
    "\n",
    "Trans=TestTransform(224, 224)\n",
    "\n",
    "import mxnet.image as mximg\n",
    "img=mximg.imread(query_img_path[5])\n",
    "img=Trans(img)\n",
    "plt.imshow(img.asnumpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract feature completed.time cost :6.60419e-05\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start=time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract feature completed.time cost :49.886\n"
     ]
    }
   ],
   "source": [
    "print (\"extract feature completed.time cost :{:.5}\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
