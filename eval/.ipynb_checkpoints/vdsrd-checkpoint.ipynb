{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T07:27:41.229305Z",
     "start_time": "2021-05-19T07:27:37.451511Z"
    },
    "code_folding": [
     45,
     51
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training statistics:\n",
      "  ------------------------------\n",
      "  trainning name  |  pr_dem\n",
      "  ------------------------------\n",
      "  num of channels |     1\n",
      "  ------------------------------\n",
      "  num of threads  |     0\n",
      "  ------------------------------\n",
      "  batch_size     |    44\n",
      "  ------------------------------\n",
      "  using cpu only |     0\n",
      "(11, 316, 376) (316, 376)\n",
      "(11, 316, 376) (316, 376)\n",
      "(11, 316, 376) (316, 376)\n",
      "(11, 316, 376) (316, 376)\n",
      "(11, 316, 376) (316, 376)\n",
      "(11, 316, 376) (316, 376)\n",
      "(11, 316, 376) (316, 376)\n",
      "(11, 316, 376) (316, 376)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-79ce4858ff02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;31m#     main(year=2010)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;31m#     print('2010done')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2012\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2012done'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-79ce4858ff02>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(year)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[1;31m#         test_data=tqdm.tqdm(test_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdem\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0men\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_time\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[0mpr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdem\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhr\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdem\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import util.data_processing_tool as dpt\n",
    "from model import vdsr\n",
    "\n",
    "from datetime import timedelta, date, datetime\n",
    "# import args_parameter as args\n",
    "import torch,torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import properscoring as ps\n",
    "from torch.utils.data import Dataset,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import time\n",
    "import xarray as xr\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import matplotlib as plt\n",
    "import argparse\n",
    "import sys\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import platform\n",
    "from datetime import timedelta, date, datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from math import log10\n",
    "import time\n",
    "# from PrepareData import ACCESS_BARRA_crps\n",
    "import tqdm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "# ===========================================================\n",
    "# Training settings\n",
    "# ===========================================================\n",
    "\n",
    "\n",
    "def rmse(ens,hr):\n",
    "    '''\n",
    "    ens:(ensemble,H,W)\n",
    "    hr: (H,W)\n",
    "    '''\n",
    "    return np.sqrt((ens-hr).sum(axis=(0)))\n",
    "\n",
    "def mae(ens,hr):\n",
    "    '''\n",
    "    ens:(ensemble,H,W)\n",
    "    hr: (H,W)\n",
    "    '''\n",
    "    return np.abs((ens-hr)).sum(axis=0)\n",
    "\n",
    "\n",
    "class ACCESS_BARRA_crps(Dataset):\n",
    "    '''\n",
    "\n",
    "2.using my net to train one channel to one channel.\n",
    "   \n",
    "    '''\n",
    "    def __init__(self,start_date=date(1990, 1, 1),end_date=date(1990,12 , 31),regin=\"AUS\",lr_transform=None,hr_transform=None,shuffle=True,args=None):\n",
    "#         print(\"=> BARRA_R & ACCESS_S1 loading\")\n",
    "#         print(\"=> from \"+start_date.strftime(\"%Y/%m/%d\")+\" to \"+end_date.strftime(\"%Y/%m/%d\")+\"\")\n",
    "        self.file_BARRA_dir = args.file_BARRA_dir\n",
    "        self.file_ACCESS_dir = args.file_ACCESS_dir\n",
    "        self.args=args\n",
    "        \n",
    "        self.lr_transform = lr_transform\n",
    "        self.hr_transform = hr_transform\n",
    "\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        \n",
    "        self.regin = regin\n",
    "        self.leading_time_we_use=args.leading_time_we_use\n",
    "\n",
    "        self.ensemble_access=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "        self.ensemble=[]\n",
    "        for i in range(args.ensemble):\n",
    "            self.ensemble.append(self.ensemble_access[i])\n",
    "                \n",
    "        self.dates = self.date_range(start_date, end_date)\n",
    "        \n",
    "        \n",
    "        self.filename_list=self.get_filename_with_time_order(args.file_ACCESS_dir+\"pr/daily/\")\n",
    "#         if not os.path.exists(args.file_ACCESS_dir+\"pr/daily/\"):\n",
    "#             print(args.file_ACCESS_dir+\"pr/daily/\")\n",
    "#             print(\"no file or no permission\")\n",
    "        \n",
    "        \n",
    "        _,_,date_for_BARRA,time_leading=self.filename_list[0]\n",
    "        if shuffle:\n",
    "            random.shuffle(self.filename_list)\n",
    "        \n",
    "        \n",
    "#         if not os.path.exists(\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/1990/01/accum_prcp-fc-spec-PT1H-BARRA_R-v1-19900109T0600Z.sub.nc\"):\n",
    "#             print(self.file_BARRA_dir)\n",
    "#             print(\"no file or no permission!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        data_high=dpt.read_barra_data_fc_get_lat_lon(self.file_BARRA_dir,date_for_BARRA)\n",
    "        self.lat=data_high[1]\n",
    "        self.lon=data_high[1]\n",
    "#         self.shape=(316, 376)\n",
    "\n",
    "\n",
    "#         print(type(self.data_dem))\n",
    "        \n",
    "#             data_dem=dpt.add_lat_lon( dpt.read_dem(args.file_DEM_dir+\"dem-9s1.tif\"))\n",
    "#             self.dem_data=dpt.interp_tensor_2d(dpt.map_aust_old(data_dem,xrarray=False) ,self.shape )\n",
    "        \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filename_list)\n",
    "    \n",
    "\n",
    "    def date_range(self,start_date, end_date):\n",
    "        \"\"\"This function takes a start date and an end date as datetime date objects.\n",
    "        It returns a list of dates for each date in order starting at the first date and ending with the last date\"\"\"\n",
    "        return [start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "    \n",
    "    def get_filename_with_no_time_order(self,rootdir):\n",
    "        '''get filename first and generate label '''\n",
    "        _files = []\n",
    "        list = os.listdir(rootdir) #åˆ—å‡ºæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰çš„ç›®å½•ä¸Žæ–‡ä»¶\n",
    "        for i in range(0,len(list)):\n",
    "            path = os.path.join(rootdir,list[i])\n",
    "            if os.path.isdir(path):\n",
    "                _files.extend(self.get_filename_with_no_time_order(path))\n",
    "            if os.path.isfile(path):\n",
    "                if path[-3:]==\".nc\":\n",
    "                    _files.append(path)\n",
    "        return _files\n",
    "    \n",
    "    def get_filename_with_time_order(self,rootdir):\n",
    "        '''get filename first and generate label ,one different w'''\n",
    "        _files = []\n",
    "        for date in self.dates:\n",
    "            for i in range(self.leading_time_we_use,self.leading_time_we_use+1):\n",
    "            \n",
    "\n",
    "#                 filename=\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"cd\n",
    "#                 access_path=rootdir+en+\"/\"+\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "#                 print(access_path)\n",
    "                for en in self.ensemble:\n",
    "                    access_path=rootdir+en+\"/\"+\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "                    if os.path.exists(access_path):\n",
    "                        \n",
    "                    \n",
    "                        if date==self.end_date and i==1:\n",
    "                            break\n",
    "                        path=[]\n",
    "                        path.append(en)\n",
    "                        barra_date=date+timedelta(i)\n",
    "                        path.append(date)\n",
    "                        path.append(barra_date)\n",
    "                        path.append(i)\n",
    "                        _files.append(path)\n",
    "\n",
    "#     def get_filename_with_time_order(self,rootdir):\n",
    "#         '''get filename first and generate label ,one different w'''\n",
    "#         _files = []\n",
    "#         for date in self.dates:\n",
    "#             for en in self.ensemble:\n",
    "\n",
    "# #                 filename=\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"cd\n",
    "#                 access_path=rootdir+en+\"/\"+\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "# #                 print(access_path)\n",
    "#                 if os.path.exists(access_path):\n",
    "#                     for i in range(self.leading_time_we_use):\n",
    "#                         if date==self.end_date and i==1:\n",
    "#                             break\n",
    "#                         path=[]\n",
    "#                         path.append(en)\n",
    "#                         barra_date=date+timedelta(i)\n",
    "#                         path.append(date)\n",
    "#                         path.append(barra_date)\n",
    "#                         path.append(i)\n",
    "#                         _files.append(path)                        \n",
    "\n",
    "    #æœ€åŽåŽ»æŽ‰ç¬¬ä¸€è¡Œï¼Œç„¶åŽshuffle\n",
    "        return _files\n",
    "\n",
    "    def mapping(self,X,min_val=0.,max_val=255.):\n",
    "        Xmin = np.min(X)\n",
    "        Xmax = np.max(X)\n",
    "        #å°†æ•°æ®æ˜ å°„åˆ°[-1,1]åŒºé—´ å³a=-1ï¼Œb=1\n",
    "        a = min_val\n",
    "        b = max_val\n",
    "        Y = a + (b-a)/(Xmax-Xmin)*(X-Xmin)\n",
    "        return Y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        from filename idx get id\n",
    "        return lr,hr\n",
    "        '''\n",
    "        t=time.time()\n",
    "#         data[var_name][:7,82:144,134:188]*86400\n",
    "        #read_data filemame[idx]\n",
    "        en,access_date,barra_date,time_leading=self.filename_list[idx]\n",
    "        \n",
    "        lr=dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"pr\")*86400\n",
    "\n",
    "#         lr=np.expand_dims(lr,axis=2)\n",
    "        lr=dpt.interp_tensor_2d(lr,(79,94))\n",
    "#         lr=np.expand_dims(self.mapping(lr),axis=2)\n",
    "        label=dpt.read_barra_data_fc(self.file_BARRA_dir,barra_date)\n",
    "\n",
    "        if self.args.zg:\n",
    "            lr_zg=np.expand_dims(dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"zg\"),axis=2)\n",
    "\n",
    "        if self.args.psl:\n",
    "            lr_psl=dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"psl\")\n",
    "\n",
    "        if self.args.tasmax:\n",
    "            lr_tasmax=np.expand_dims(dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"tasmax\"),axis=2)\n",
    "\n",
    "\n",
    "        if self.args.tasmin:\n",
    "            lr_tasmin=dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"tasmin\")\n",
    "            \n",
    "#         if self.args.channels==1:\n",
    "#             lr=np.repeat(lr,3,axis=2)\n",
    "#         return self.lr_transform(Image.fromarray(lr)),self.lr_transform(Image.fromarray(self.data_dem)),self.hr_transform(Image.fromarray(label))\n",
    "        \n",
    "        return self.lr_transform(Image.fromarray(lr)),torch.tensor(int(en[1:])),self.hr_transform(Image.fromarray(label)),torch.tensor(int(en[1:])),torch.tensor(int(access_date.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def write_log(log,args):\n",
    "    print(log)\n",
    "    if not os.path.exists(\"./save/\"+args.train_name+\"/\"):\n",
    "        os.mkdir(\"./save/\"+args.train_name+\"/\")\n",
    "    my_log_file=open(\"./save/\"+args.train_name + '/train.txt', 'a')\n",
    "#     log=\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time())\n",
    "    my_log_file.write(log + '\\n')\n",
    "    my_log_file.close()\n",
    "    return\n",
    " \n",
    "def main(year):\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Super Res Example')\n",
    "    # Hardware specifications\n",
    "    parser.add_argument('--n_threads', type=int, default=0,\n",
    "                        help='number of threads for data loading')\n",
    "\n",
    "    parser.add_argument('--cpu', action='store_true',help='cpu only?') \n",
    "\n",
    "    # hyper-parameters\n",
    "    parser.add_argument('--train_name', type=str, default=\"cali_crps\", help='training name')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=44, help='training batch size')\n",
    "    parser.add_argument('--testBatchSize', type=int, default=4, help='testing batch size')\n",
    "    parser.add_argument('--nEpochs', type=int, default=200, help='number of epochs to train for')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001, help='Learning Rate. Default=0.01')\n",
    "    parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\n",
    "\n",
    "    # model configuration\n",
    "    parser.add_argument('--upscale_factor', '-uf',  type=int, default=4, help=\"super resolution upscale factor\")\n",
    "    parser.add_argument('--model', '-m', type=str, default='vdsr', help='choose which model is going to use')\n",
    "\n",
    "    #data\n",
    "    parser.add_argument('--pr', type=bool, default=True,help='add-on pr?')\n",
    "\n",
    "    parser.add_argument('--train_start_time', type=type(datetime(1990,1,25)), default=datetime(1990,1,2),help='r?')\n",
    "    parser.add_argument('--train_end_time', type=type(datetime(1990,1,25)), default=datetime(1990,2,9),help='?')\n",
    "    parser.add_argument('--test_start_time', type=type(datetime(2012,1,1)), default=datetime(2010,1,1),help='a?')\n",
    "    parser.add_argument('--test_end_time', type=type(datetime(2012,12,31)), default=datetime(2010,12,31),help='')\n",
    "\n",
    "    parser.add_argument('--dem', action='store_true',help='add-on dem?') \n",
    "    parser.add_argument('--psl', action='store_true',help='add-on psl?') \n",
    "    parser.add_argument('--zg', action='store_true',help='add-on zg?') \n",
    "    parser.add_argument('--tasmax', action='store_true',help='add-on tasmax?') \n",
    "    parser.add_argument('--tasmin', action='store_true',help='add-on tasmin?')\n",
    "    parser.add_argument('--leading_time_we_use', type=int,default=1\n",
    "                        ,help='add-on tasmin?')\n",
    "    parser.add_argument('--ensemble', type=int, default=11,help='total ensambles is 11') \n",
    "    parser.add_argument('--channels', type=float, default=0,help='channel of data_input must') \n",
    "    #[111.85, 155.875, -44.35, -9.975]\n",
    "    parser.add_argument('--domain', type=list, default=[112.9, 154.25, -43.7425, -9.0],help='dataset directory')\n",
    "\n",
    "#     parser.add_argument('--file_ACCESS_dir', type=str, default=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\",help='dataset directory')\n",
    "    parser.add_argument('--file_ACCESS_dir', type=str, default=\"../../Data/\",help='dataset directory')\n",
    "\n",
    "    parser.add_argument('--file_BARRA_dir', type=str, default=\"../../Data/barra_aus/\",help='dataset directory')\n",
    "    parser.add_argument('--file_DEM_dir', type=str, default=\"../DEM/\",help='dataset directory')\n",
    "    parser.add_argument('--precision', type=str, default='single',choices=('single', 'half','double'),help='FP precision for test (single | half)')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    \n",
    "    # def main():\n",
    "\n",
    "#     init_date=date(1970, 1, 1)\n",
    "#     start_date=date(1990, 1, 2)\n",
    "#     end_date=date(2011,12,25)\n",
    "    sys = platform.system()\n",
    "    args.dem=False\n",
    "    args.train_name=\"pr_dem\"\n",
    "    args.channels=0\n",
    "    if args.pr:\n",
    "        args.channels+=1\n",
    "    if args.zg:\n",
    "        args.channels+=1\n",
    "    if args.psl:\n",
    "        args.channels+=1\n",
    "    if args.tasmax:\n",
    "        args.channels+=1\n",
    "    if args.tasmin:\n",
    "        args.channels+=1\n",
    "    if args.dem:\n",
    "        args.channels+=1\n",
    "    print(\"training statistics:\")\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  trainning name  |  %s\"%args.train_name)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of channels | %5d\"%args.channels)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  batch_size     | %5d\"%args.batch_size)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  using cpu only | %5d\"%args.cpu)\n",
    "\n",
    "    lr_transforms = transforms.Compose([\n",
    "        transforms.Resize((316, 376)),\n",
    "    #     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(30),\n",
    "        transforms.ToTensor()\n",
    "    #     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "\n",
    "    hr_transforms = transforms.Compose([\n",
    "    #         transforms.Resize((316, 376)),\n",
    "    #     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(30),\n",
    "        transforms.ToTensor()\n",
    "    #     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "    args.test_start_time=datetime(year,1,1)\n",
    "    args.test_end_time=datetime(year,12,31)\n",
    "    data_set=ACCESS_BARRA_crps(args.test_start_time,args.test_end_time,lr_transform=lr_transforms,hr_transform=hr_transforms,shuffle=False,args=args)\n",
    "\n",
    "\n",
    "\n",
    "    #     #######################################################################\n",
    "\n",
    "    test_data=DataLoader(data_set,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=False,\n",
    "                                num_workers=args.n_threads,drop_last=True)\n",
    "\n",
    "    #     #######################################################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if year==2012:\n",
    "        net=torch.load('../save/model/vdsr_pr/best_test.pth',map_location=device)\n",
    "    else:\n",
    "        net=torch.load('../save/model/vdsr_pr/best_test.pth',map_location=device)['model']\n",
    "\n",
    "    def prepare( l, device=False):\n",
    "        def _prepare(tensor):\n",
    "            if args.precision == 'half': tensor = tensor.half()\n",
    "            if args.precision == 'single': tensor = tensor.float()\n",
    "            return tensor.to(device)\n",
    "\n",
    "        return [_prepare(_l) for _l in l]\n",
    "\n",
    "\n",
    "    #     ##############################################\n",
    "\n",
    "    #     max_error=np.inf\n",
    "    #     val_max_error=np.inf\n",
    "\n",
    "    #     print(data_set.filename_list)\n",
    "\n",
    "    # for e in range(args.nEpochs):\n",
    "    #         loss=0\n",
    "    for lead in range(217):\n",
    "        args.leading_time_we_use=lead\n",
    "\n",
    "        data_set=ACCESS_BARRA_crps(args.test_start_time,args.test_end_time,lr_transform=lr_transforms,hr_transform=hr_transforms,shuffle=False,args=args)\n",
    "\n",
    "\n",
    "        test_data=DataLoader(data_set,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                shuffle=False,\n",
    "                                    num_workers=args.n_threads,drop_last=False)\n",
    "\n",
    "\n",
    "        crps_score_vsdr=[]\n",
    "        mae_score_vsdr=[]\n",
    "        rmse_score_vsdr=[]\n",
    "        start=time.time()\n",
    "        fmt = '%Y%m%d'\n",
    "\n",
    "\n",
    "    #         test_data=tqdm.tqdm(test_data)\n",
    "        for batch, (pr,dem,hr,en,data_time,idx) in enumerate(test_data):\n",
    "            pr,dem,hr= prepare([pr,dem,hr],device)\n",
    "            with torch.set_grad_enabled(False):\n",
    "\n",
    "#                 sr = net(pr)\n",
    "                sr_np=pr.cpu().numpy()\n",
    "                hr_np=hr.cpu().numpy()\n",
    "#                 print(pr.shape)\n",
    "#                 print(hr.shape)\n",
    "#                 print(en)\n",
    "#                 print(data_time)\n",
    "                for i in range(args.batch_size//args.ensemble):\n",
    "                    a=np.squeeze( sr_np[i*args.ensemble:(i+1)*args.ensemble])\n",
    "                    b=np.squeeze(hr_np[i*args.ensemble])\n",
    "                    print(a.shape,b.shape)\n",
    "                    #skil=vectcrps_m(a,b)\n",
    "#                    skil=ps.crps_ensemble(b,np.transpose(a,(1,2,0)))\n",
    "                    rmes_score=rmse(a,b)\n",
    "                    mae_score=mae(a,b)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    #crps_score_vsdr.append(skil)\n",
    "                    mae_score_vsdr.append(mae_score)\n",
    "                    rmse_score_vsdr.append(rmes_score)\n",
    "        if not os.path.exists(\"../save/rmse/vdsrd/\"+str(year)):\n",
    "            dpt.mkdir(\"../save/rmse/vdsrd/\"+str(year))\n",
    "            \n",
    "        if not os.path.exists(\"../save/mae/vdsrd/\"+str(year)):\n",
    "            dpt.mkdir(\"../save/mae/vdsrd/\"+str(year))\n",
    "            \n",
    "        if not os.path.exists(\"../save/crps/vdsrd/\"+str(year)):\n",
    "            dpt.mkdir(\"../save/crps/vdsrd/\"+str(year))\n",
    "#         np.save(\"../save/crps/bi_217/2010/lead_time_\"+str(lead),crps_score_vsdr)\n",
    "        np.save(\"../save/rmse/vdsrd/\"+str(year)+\"/lead_time_\"+str(lead),rmse_score_vsdr)\n",
    "        np.save(\"../save/mae/vdsrd/\"+str(year)+\"/lead_time_\"+str(lead),mae_score_vsdr)\n",
    "        \n",
    "        print(str(lead)+\" : \"+str(np.array(rmse_score_vsdr).mean()),\",\")\n",
    "        print(str(lead)+\" : \"+str(np.array(mae_score_vsdr).mean()),\",\")\n",
    "            \n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "#     main(year=1997) \n",
    "#     print('1997done')\n",
    "#     main(year=2010) \n",
    "#     print('2010done')\n",
    "    main(year=2012)\n",
    "    print('2012done')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T06:42:24.275172Z",
     "start_time": "2021-05-19T06:42:24.269598Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 94)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dpt.read_access_data('../../Data/','e01',date(2012,1,1),0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T04:20:46.401618Z",
     "start_time": "2021-05-19T04:20:45.882953Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net=torch.load('../save/model/vdsr_pr/best_test.pth',map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import util.PrepareData as pdata\n",
    "import util.data_processing_tool as dpt\n",
    "\n",
    "from datetime import date\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from datetime import timedelta, date, datetime\n",
    "import platform \n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import util.constant_param as consparam\n",
    "start_date=date(2012, 1, 1)\n",
    "end_date=date(2012,12,25)\n",
    "dates=[start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "\n",
    "\n",
    "def get_filename_with_no_time_order(rootdir):\n",
    "    '''get filename first and generate label '''\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i],)\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(get_filename_with_no_time_order(path))\n",
    "        if os.path.isfile(path):\n",
    "            if path[-3:]==\".nc\":\n",
    "                _files.append(path)\n",
    "    return _files\n",
    "\n",
    "\n",
    "def get_filename_with_time_order(rootdir,ensemble,dates,var_name):\n",
    "    '''get filename first and generate label ,one different w'''\n",
    "    _files = []\n",
    "    for en in ensemble:\n",
    "        for date in dates:\n",
    "            access_path=rootdir+var_name+'/daily/'+en+\"/\"+\"da_\"+var_name+\"_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "#             print(access_path)\n",
    "            if os.path.exists(access_path):\n",
    "#                 print(access_path)\n",
    "                path=[access_path]\n",
    "                path.append(en)\n",
    "                path.append(date)\n",
    "                _files.append(path)\n",
    "\n",
    "#最后去掉第一行，然后shuffle\n",
    "#     if nine2nine and date_minus_one==1:\n",
    "#         del _files[0]\n",
    "    return _files\n",
    "\n",
    "\n",
    "#50 station\n",
    "def get_access_SR_data():# 顺便保存 BI数据\n",
    "    \n",
    "    \n",
    "    lr_transforms = transforms.Compose([\n",
    "    transforms.Resize((316, 376)),\n",
    "#     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(30),\n",
    "    transforms.ToTensor()\n",
    "#     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    import netCDF4 as nc\n",
    "#     ensemble=['e01','e02']\n",
    "    sys = platform.system()\n",
    "    init_date=date(1970, 1, 1)\n",
    "    start_date=date(2012, 1, 1)\n",
    "    end_date=date(2012,12,25) #if 929 is true we should substract 1 day  \n",
    "    if sys == \"Windows\":\n",
    "        init_date=date(1970, 1, 1)\n",
    "        start_date=date(1990, 1, 1)\n",
    "        end_date=date(2012,12,25) #if 929 is true we should substract 1 day   \n",
    "        file_ACCESS_dir=\"H:/climate/access-s1/\" \n",
    "        file_BARRA_dir=\"D:/dataset/accum_prcp/\"\n",
    "    #         args.file_ACCESS_dir=\"E:/climate/access-s1/\"\n",
    "    #         args.file_BARRA_dir=\"C:/Users/JIA059/barra/\"\n",
    "        file_DEM_dir=\"../DEM/\"\n",
    "    else:\n",
    "        file_ACCESS_dir_pr=\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "        file_ACCESS_dir=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\"\n",
    "        # training_name=\"temp01\"\n",
    "        file_BARRA_dir=\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/\"\n",
    "\n",
    "        \n",
    "    ensemble=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "\n",
    "    var_name=\"pr\"\n",
    "    dates=[start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "    file_list=get_filename_with_time_order(file_ACCESS_dir+var_name+'/daily/',ensemble,dates,var_name)\n",
    "    time_leading=217\n",
    "\n",
    "    lat_name=\"lat\"\n",
    "    lon_name=\"lon\"\n",
    "\n",
    "#     print(file_list)\n",
    "    for i in file_list:\n",
    "#         print(i)\n",
    "        data = Dataset(i[0], 'r')\n",
    "        var = data[var_name][:,82:144,134:188]*86400\n",
    "        lat = data[lat_name][:][82:144]\n",
    "        lon = data[lon_name][:][134:188]\n",
    "#         print(var.shape)\n",
    "        data.close()\n",
    "    #         lr=dpt.read_access_data(i,idx=time_leading).data[82:144,134:188]*86400\n",
    "        result= torch.zeros((217,1,316,376),dtype=torch.float32)\n",
    "\n",
    "        for idx,j in enumerate(var):\n",
    "            if idx>= 217:\n",
    "                break\n",
    "            lr=dpt.interp_tensor_2d(j,(79,94))\n",
    "            result[idx]=lr_transforms(Image.fromarray(lr))\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        net=torch.load('../save/model/vdsr_pr/best_test.pth',map_location=device)\n",
    "        def prepare( l, device=False):\n",
    "            def _prepare(tensor):\n",
    "                tensor = tensor.float()\n",
    "                return tensor.to(device)\n",
    "            return [_prepare(_l) for _l in l]\n",
    "        llrr=prepare([result],device)[0]\n",
    "        with torch.set_grad_enabled(False):\n",
    "            sr=net(llrr)\n",
    "        sr_np=np.squeeze(sr.cpu().numpy())\n",
    "        \n",
    "        data_50=[]\n",
    "        station_name=[]\n",
    "        for station in consparam.station_50_index_for_size_of_hr_sr_station_code.keys():\n",
    "            idx_i=consparam.station_50_index_for_size_of_hr_sr_station_code[station][0]\n",
    "            idx_j=consparam.station_50_index_for_size_of_hr_sr_station_code[station][1]\n",
    "            data_50.append(sr_np[:,idx_i,idx_j])\n",
    "            station_name.append(station)\n",
    "        data_50=np.array(data_50)\n",
    "        print(data_50.shape)\n",
    "        station_name=np.array(station_name)\n",
    "        \n",
    "        if not os.path.exists('../Data/'+var_name+'/daily_vdsrd_50station/'+i[1]):\n",
    "            os.makedirs('../Data/'+var_name+'/daily_vdsrd_50station/'+i[1])\n",
    "        \n",
    "        f_w = nc.Dataset('../Data/'+var_name+'/daily_vdsrd_50station/'+i[1]+\"/da_\"+var_name+\"_\"+i[2].strftime(\"%Y%m%d\")+\"_\"+i[1]+'.nc','w',format = 'NETCDF4')\n",
    "\n",
    "        f_w.createDimension('time',time_leading)\n",
    "        f_w.createDimension('station',50)\n",
    "\n",
    "        f_w.createVariable('station',np.int32,('station'))\n",
    "        f_w.createVariable('time',np.int,('time'))\n",
    "\n",
    "\n",
    "\n",
    "        f_w.variables['station'][:] = station_name\n",
    "        f_w.variables['time'][:] =  np.array(range(1,218))\n",
    "\n",
    "\n",
    "        f_w.createVariable( 'pr', np.float32, ('station','time'))\n",
    "        f_w.variables['pr'][:] = data_50\n",
    "\n",
    "        f_w.close()\n",
    "            \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    get_access_SR_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
