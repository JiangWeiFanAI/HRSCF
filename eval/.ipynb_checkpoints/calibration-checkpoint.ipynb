{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-19T06:56:34.508Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import util.data_processing_tool as dpt\n",
    "from model import vdsr\n",
    "\n",
    "from datetime import timedelta, date, datetime\n",
    "# import args_parameter as args\n",
    "import torch,torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import properscoring as ps\n",
    "from torch.utils.data import Dataset,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import time\n",
    "import xarray as xr\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import matplotlib as plt\n",
    "import argparse\n",
    "import sys\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import platform\n",
    "from datetime import timedelta, date, datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from math import log10\n",
    "import time\n",
    "# from PrepareData import ACCESS_BARRA_crps\n",
    "import tqdm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "# ===========================================================\n",
    "# Training settings\n",
    "# ===========================================================\n",
    "\n",
    "\n",
    "def rmse(ens,hr):\n",
    "    '''\n",
    "    ens:(ensemble,H,W)\n",
    "    hr: (H,W)\n",
    "    '''\n",
    "    return np.sqrt((ens-hr).sum(axis=(0)))\n",
    "\n",
    "def mae(ens,hr):\n",
    "    '''\n",
    "    ens:(ensemble,H,W)\n",
    "    hr: (H,W)\n",
    "    '''\n",
    "    return np.abs((ens-hr)).sum(axis=0)\n",
    "\n",
    "\n",
    "class ACCESS_BARRA_cali(Dataset):\n",
    "    '''\n",
    "\n",
    "2.using my net to train one channel to one channel.\n",
    "   \n",
    "    '''\n",
    "    def __init__(self,start_date=date(2012, 1, 1),end_date=date(2012,12 , 31),regin=\"AUS\",lr_transform=None,hr_transform=None,shuffle=True,args=None):\n",
    "#         print(\"=> BARRA_R & ACCESS_S1 loading\")\n",
    "#         print(\"=> from \"+start_date.strftime(\"%Y/%m/%d\")+\" to \"+end_date.strftime(\"%Y/%m/%d\")+\"\")\n",
    "        self.file_BARRA_dir = args.file_BARRA_dir\n",
    "        self.file_ACCESS_dir = '/g/data/ub7/access-s1/hc/calibrated_5km_v3/atmos/pr/'\n",
    "        self.args=args\n",
    "        \n",
    "        self.lr_transform = lr_transform\n",
    "        self.hr_transform = hr_transform\n",
    "\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        \n",
    "        self.regin = regin\n",
    "        self.leading_time_we_use=args.leading_time_we_use\n",
    "\n",
    "        self.ensemble_access=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "        self.ensemble=[]\n",
    "        for i in range(args.ensemble):\n",
    "            self.ensemble.append(self.ensemble_access[i])\n",
    "                \n",
    "        self.dates = self.date_range(start_date, end_date)\n",
    "        \n",
    "        \n",
    "        self.filename_list=self.get_filename_with_time_order(self.file_ACCESS_dir+\"/daily/\")\n",
    "        if not os.path.exists(args.file_ACCESS_dir+\"daily/\"):\n",
    "            print(args.file_ACCESS_dir+\"daily/\")\n",
    "            print(\"no file or no permission\")\n",
    "        \n",
    "        \n",
    "        en,cali_date,date_for_BARRA,time_leading=self.filename_list[0]\n",
    "        if shuffle:\n",
    "            random.shuffle(self.filename_list)\n",
    "\n",
    "        data_barra=dpt.read_barra_data_fc_get_lat_lon(self.file_BARRA_dir,date_for_BARRA)\n",
    "        self.lat_barra=data_barra[1]\n",
    "        self.lon_barra=data_barra[2]\n",
    "        \n",
    "        \n",
    "        data_cali=dpt.read_access_data_calibrataion_get_lat_lon(self.file_ACCESS_dir,en,cali_date,0)\n",
    "        self.lat_cali=data_cali[1]\n",
    "        self.lon_cali=data_cali[2]\n",
    "#         self.shape=(316, 376)\n",
    "\n",
    "#         self.data_dem=dpt.read_dem(args.file_DEM_dir+\"dem-9s1.tif\")\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filename_list)\n",
    "    \n",
    "\n",
    "    def date_range(self,start_date, end_date):\n",
    "        \"\"\"This function takes a start date and an end date as datetime date objects.\n",
    "        It returns a list of dates for each date in order starting at the first date and ending with the last date\"\"\"\n",
    "        return [start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "    \n",
    "    def get_filename_with_no_time_order(self,rootdir):\n",
    "        '''get filename first and generate label '''\n",
    "        _files = []\n",
    "        list = os.listdir(rootdir) #åˆ—å‡ºæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰çš„ç›®å½•ä¸Žæ–‡ä»¶\n",
    "        for i in range(0,len(list)):\n",
    "            path = os.path.join(rootdir,list[i])\n",
    "            if os.path.isdir(path):\n",
    "                _files.extend(self.get_filename_with_no_time_order(path))\n",
    "            if os.path.isfile(path):\n",
    "                if path[-3:]==\".nc\":\n",
    "                    _files.append(path)\n",
    "        return _files\n",
    "    \n",
    "    def get_filename_with_time_order(self,rootdir):\n",
    "        '''get filename first and generate label ,one different w'''\n",
    "        _files = []\n",
    "        for date in self.dates:\n",
    "            for i in range(self.leading_time_we_use,self.leading_time_we_use+1):\n",
    "            \n",
    "                for en in self.ensemble:\n",
    "                    access_path=rootdir+en+\"/\"+\"daq5_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "                    if os.path.exists(access_path):\n",
    "                        \n",
    "                    \n",
    "                        if date==self.end_date and i==1:\n",
    "                            break\n",
    "                        path=[]\n",
    "                        path.append(en)\n",
    "                        barra_date=date+timedelta(i)\n",
    "                        path.append(date)\n",
    "                        path.append(barra_date)\n",
    "                        path.append(i)\n",
    "                        _files.append(path)\n",
    "                    \n",
    "\n",
    "    #æœ€åŽåŽ»æŽ‰ç¬¬ä¸€è¡Œï¼Œç„¶åŽshuffle\n",
    "        return _files\n",
    "\n",
    "    def mapping(self,X,min_val=0.,max_val=255.):\n",
    "        Xmin = np.min(X)\n",
    "        Xmax = np.max(X)\n",
    "        #å°†æ•°æ®æ˜ å°„åˆ°[-1,1]åŒºé—´ å³a=-1ï¼Œb=1\n",
    "        a = min_val\n",
    "        b = max_val\n",
    "        Y = a + (b-a)/(Xmax-Xmin)*(X-Xmin)\n",
    "        return Y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        from filename idx get id\n",
    "        return lr,hr\n",
    "        '''\n",
    "        t=time.time()\n",
    "        \n",
    "        #read_data filemame[idx]\n",
    "        en,access_date,barra_date,time_leading=self.filename_list[idx]\n",
    "        \n",
    "\n",
    "        lr=dpt.read_access_data_calibrataion(self.file_ACCESS_dir,en,access_date,time_leading,\"pr\")\n",
    "        label=dpt.read_barra_data_fc(self.file_BARRA_dir,barra_date)\n",
    "\n",
    "        return np.array(lr),np.array([1]),self.hr_transform(Image.fromarray(label)),torch.tensor(int(en[1:])),torch.tensor(int(access_date.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def crps(ensin,obs):\n",
    "    '''\n",
    "    @param ensin A vector of prediction\n",
    "    @param obs  A vector of observations\n",
    "    \n",
    "'''\n",
    "\n",
    "#     assert not np.isnan(ensin).any() and not np.isnan(obs).any(), \"data contains nan\"\n",
    "         \n",
    "    Fn = ECDF(ensin)\n",
    "    xn=np.sort(np.unique(ensin))\n",
    "    m=len(xn)\n",
    "    dn=np.diff(xn)\n",
    "    eq1=0\n",
    "    eq2=0\n",
    "    if(obs>xn[0] and obs<xn[m-1]): #obsÃ¥Å“Â¨Ã¨Å’Æ’Ã¥â€ºÂ´Ã¥â€ â€¦\n",
    "        k=np.max(np.where(xn<=obs))#Ã¥Â°ÂÃ¤ÂºÅ½obsÃ§Å¡â€žÃ¦Å“â‚¬Ã¥Â¤Â§Ã¥â‚¬Â¼Ã¤Â¸â€¹Ã¦Â â€¡\n",
    "        x0 = xn[k] #Ã¥Â°ÂÃ¤ÂºÅ½obsÃ§Å¡â€žÃ¦Å“â‚¬Ã¥Â¤Â§Ã¥â‚¬Â¼\n",
    "        if k>0:\n",
    "            eq1=np.sum(Fn(xn[0:k+1])**2*np.append(dn[0:k], obs - xn[k]))#Ã¥Â°ÂÃ¤ÂºÅ½obsÃ§Å¡â€žÃ¦â€°â‚¬Ã¦Å“â€°Ã¥â‚¬Â¼ Ã§Å¡â€ž Ã§â„¢Â¾Ã¥Ë†â€ Ã¦Â¯â€Ã¦â€¢Â° Ã§Å¡â€žÃ¥Â¹Â³Ã¦â€“Â¹\n",
    "        else:\n",
    "            eq1 =np.sum(Fn(xn[0])**2*(obs - xn[0]))\n",
    "        if k<m-2:\n",
    "\n",
    "            eq2=np.sum((1-Fn(xn[k:m-1]))**2*np.append(xn[k+1] - obs, dn[(k+1):(m-1)]))\n",
    "        else:\n",
    "            eq2 =np.sum((1-Fn(xn[m-2]))**2*(xn[m-1] - obs))\n",
    "\n",
    "    if obs <= xn[0]: # Ã¨Â§â€šÃ¦Âµâ€¹Ã¥â‚¬Â¼Ã¥Å“Â¨Ã¤Â¹â€¹Ã¥Â¤â€“\n",
    "        eq2 =np.sum(np.append(1, 1-Fn(xn[0:(m-1)]))**2*np.append(xn[0]-obs, dn))\n",
    "    if obs >= xn[m-1]:\n",
    "        eq1= np.sum(Fn(xn)**2*np.append(dn, obs - xn[m-1]))\n",
    "            \n",
    "    return eq1+eq2 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vectcrps_v(fct_ens,obs):\n",
    "    '''\n",
    "    #' @param fct_ens A 2D prediction\n",
    "    #' @param obs  A vector of observations\n",
    "    #' @return a crps vector'''\n",
    "    score =0\n",
    "\n",
    "    \n",
    "    fct_ens=fct_ens\n",
    "    assert not np.isnan(fct_ens).any() and not np.isnan(obs).any(),\"data contains nan\"\n",
    "    for i in range(obs.shape[0]):\n",
    "#         print(fct_ens[:,i],obs[i])\n",
    "        score+=crps(fct_ens[:,i],obs[i])\n",
    "  \n",
    "    return score\n",
    "\n",
    "\n",
    "def vectcrps_m(fct_ens,obs):\n",
    "    '''\n",
    "    #' @param fct_ens A 2D prediction 11*1*1\n",
    "    #' @param obs  A vector of observations\n",
    "    #' @return a crps vector'''\n",
    "    score =0\n",
    "#     assert np.isnan(fct_ens).any() and np.isnan(obs).any(),\"data contains nan\"\n",
    "    score_map=np.zeros((obs.shape[0],obs.shape[1]))\n",
    "    for i in range(obs.shape[0]):\n",
    "        for j in range(obs.shape[1]):\n",
    "            score_map[i,j]=crps(fct_ens[:,i,j],obs[i,j])\n",
    "#             score+=crps(fct_ens[:,i,j],obs[i,j])\n",
    "    return score_map\n",
    "    return score/(obs.shape[0]*obs.shape[1])   \n",
    "\n",
    "\n",
    "def get_hr_size_of_cali(fct_ens,obs):\n",
    "\n",
    "    score =0\n",
    "    mapp=np.load(\"mmap.npy\")\n",
    "#     assert np.isnan(fct_ens).any() and not np.isnan(obs).any(),\"data contains nan\"\n",
    "    score_map=np.zeros((fct_ens.shape[0],obs.shape[1],obs.shape[2]))\n",
    "    print(mapp.shape)\n",
    "    for i in range(obs.shape[1]):\n",
    "        for j in range(obs.shape[2]):\n",
    "#             if (np.array(fct_ens[:,i,j],dtype=np.float32)>2000).any() or (mapp[i,j]==-1).any():\n",
    "#             if (np.array(fct_ens[:,i,j],dtype=np.float32)>2000).any(): \n",
    "            if (np.array(fct_ens[:,mapp[i,j,0],mapp[i,j,1]],dtype=np.int)>=-0.00001).all():\n",
    "                score_map[:,i,j]=fct_ens[:,mapp[i,j,0],mapp[i,j,1]].copy()\n",
    "            else:\n",
    "                score_map[:,i,j]=np.inf\n",
    "#             score+=crps(fct_ens[:,i,j],obs[i,j])\n",
    "    return score_map\n",
    "\n",
    "\n",
    "def write_log(log,args):\n",
    "    print(log)\n",
    "    if not os.path.exists(\"./save/\"+args.train_name+\"/\"):\n",
    "        os.mkdir(\"./save/\"+args.train_name+\"/\")\n",
    "    my_log_file=open(\"./save/\"+args.train_name + '/train.txt', 'a')\n",
    "#     log=\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time())\n",
    "    my_log_file.write(log + '\\n')\n",
    "    my_log_file.close()\n",
    "    return\n",
    " \n",
    "def main(year):\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Super Res Example')\n",
    "    # Hardware specifications\n",
    "    parser.add_argument('--n_threads', type=int, default=0,\n",
    "                        help='number of threads for data loading')\n",
    "\n",
    "    parser.add_argument('--cpu', action='store_true',help='cpu only?') \n",
    "\n",
    "    # hyper-parameters\n",
    "    parser.add_argument('--train_name', type=str, default=\"cali_crps\", help='training name')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=44, help='training batch size')\n",
    "    parser.add_argument('--testBatchSize', type=int, default=4, help='testing batch size')\n",
    "    parser.add_argument('--nEpochs', type=int, default=200, help='number of epochs to train for')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001, help='Learning Rate. Default=0.01')\n",
    "    parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\n",
    "\n",
    "    # model configuration\n",
    "    parser.add_argument('--upscale_factor', '-uf',  type=int, default=4, help=\"super resolution upscale factor\")\n",
    "    parser.add_argument('--model', '-m', type=str, default='vdsr', help='choose which model is going to use')\n",
    "\n",
    "    #data\n",
    "    parser.add_argument('--pr', type=bool, default=True,help='add-on pr?')\n",
    "\n",
    "    parser.add_argument('--train_start_time', type=type(datetime(1990,1,25)), default=datetime(1990,1,2),help='r?')\n",
    "    parser.add_argument('--train_end_time', type=type(datetime(1990,1,25)), default=datetime(1990,2,9),help='?')\n",
    "    parser.add_argument('--test_start_time', type=type(datetime(2012,1,1)), default=datetime(2010,1,1),help='a?')\n",
    "    parser.add_argument('--test_end_time', type=type(datetime(2012,12,31)), default=datetime(2010,12,31),help='')\n",
    "\n",
    "    parser.add_argument('--dem', action='store_true',help='add-on dem?') \n",
    "    parser.add_argument('--psl', action='store_true',help='add-on psl?') \n",
    "    parser.add_argument('--zg', action='store_true',help='add-on zg?') \n",
    "    parser.add_argument('--tasmax', action='store_true',help='add-on tasmax?') \n",
    "    parser.add_argument('--tasmin', action='store_true',help='add-on tasmin?')\n",
    "    parser.add_argument('--leading_time_we_use', type=int,default=1\n",
    "                        ,help='add-on tasmin?')\n",
    "    parser.add_argument('--ensemble', type=int, default=11,help='total ensambles is 11') \n",
    "    parser.add_argument('--channels', type=float, default=0,help='channel of data_input must') \n",
    "    #[111.85, 155.875, -44.35, -9.975]\n",
    "    parser.add_argument('--domain', type=list, default=[112.9, 154.25, -43.7425, -9.0],help='dataset directory')\n",
    "\n",
    "    parser.add_argument('--file_ACCESS_dir', type=str, default=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\",help='dataset directory')\n",
    "    parser.add_argument('--file_BARRA_dir', type=str, default=\"../../Data/barra_aus/\",help='dataset directory')\n",
    "    parser.add_argument('--file_DEM_dir', type=str, default=\"../DEM/\",help='dataset directory')\n",
    "    parser.add_argument('--precision', type=str, default='single',choices=('single', 'half','double'),help='FP precision for test (single | half)')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # def main():\n",
    "\n",
    "#     init_date=date(1970, 1, 1)\n",
    "#     start_date=date(1990, 1, 2)\n",
    "#     end_date=date(2011,12,25)\n",
    "    sys = platform.system()\n",
    "    args.dem=False\n",
    "    args.train_name=\"pr_dem\"\n",
    "    args.channels=0\n",
    "    if args.pr:\n",
    "        args.channels+=1\n",
    "    if args.zg:\n",
    "        args.channels+=1\n",
    "    if args.psl:\n",
    "        args.channels+=1\n",
    "    if args.tasmax:\n",
    "        args.channels+=1\n",
    "    if args.tasmin:\n",
    "        args.channels+=1\n",
    "    if args.dem:\n",
    "        args.channels+=1\n",
    "    print(\"training statistics:\")\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  trainning name  |  %s\"%args.train_name)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of channels | %5d\"%args.channels)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  batch_size     | %5d\"%args.batch_size)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  using cpu only | %5d\"%args.cpu)\n",
    "\n",
    "    lr_transforms = transforms.Compose([\n",
    "        transforms.Resize((316, 376)),\n",
    "    #     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(30),\n",
    "        transforms.ToTensor()\n",
    "    #     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "\n",
    "    hr_transforms = transforms.Compose([\n",
    "    #         transforms.Resize((316, 376)),\n",
    "    #     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(30),\n",
    "        transforms.ToTensor()\n",
    "    #     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "    args.test_start_time=datetime(year,1,1)\n",
    "    args.test_end_time=datetime(year,12,31)\n",
    "    data_set=ACCESS_BARRA_crps(args.test_start_time,args.test_end_time,lr_transform=lr_transforms,hr_transform=hr_transforms,shuffle=False,args=args)\n",
    "\n",
    "\n",
    "\n",
    "    #     #######################################################################\n",
    "\n",
    "    test_data=DataLoader(data_set,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=False,\n",
    "                                num_workers=args.n_threads,drop_last=True)\n",
    "\n",
    "    #     #######################################################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # net=torch.load(\"./save/vdsr_pr/best_test.pth\")\n",
    "    # net=torch.load(\"../data/model/vdsr_pr/best_test.pth\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     ##############################################\n",
    "\n",
    "    #     max_error=np.inf\n",
    "    #     val_max_error=np.inf\n",
    "\n",
    "    #     print(data_set.filename_list)\n",
    "\n",
    "    # for e in range(args.nEpochs):\n",
    "    #         loss=0\n",
    "    for lead in range(217):\n",
    "        args.leading_time_we_use=lead\n",
    "\n",
    "        data_set=ACCESS_BARRA_crps(args.test_start_time,args.test_end_time,lr_transform=lr_transforms,hr_transform=hr_transforms,shuffle=False,args=args)\n",
    "\n",
    "\n",
    "        test_data=DataLoader(data_set,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                shuffle=False,\n",
    "                                    num_workers=args.n_threads,drop_last=False)\n",
    "\n",
    "\n",
    "        crps_score_vsdr=[]\n",
    "        mae_score_vsdr=[]\n",
    "        rmse_score_vsdr=[]\n",
    "        start=time.time()\n",
    "        fmt = '%Y%m%d'\n",
    "\n",
    "\n",
    "    #         test_data=tqdm.tqdm(test_data)\n",
    "        for batch, (pr,dem,hr,en,data_time,idx) in enumerate(test_data):\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "\n",
    "    #                 sr = net(pr)\n",
    "                sr_np=pr.cpu().numpy()\n",
    "                hr_np=hr.cpu().numpy()\n",
    "#                 print(pr.shape)\n",
    "#                 print(hr.shape)\n",
    "#                 print(en)\n",
    "#                 print(data_time)\n",
    "                for i in range(args.batch_size//args.ensemble):\n",
    "                    a=np.squeeze( sr_np[i*args.ensemble:(i+1)*args.ensemble])\n",
    "                    b=np.squeeze(hr_np[i*args.ensemble])\n",
    "                    a=get_hr_size_of_cali(a,b)\n",
    "                    #skil=vectcrps_m(a,b)\n",
    "#                    skil=ps.crps_ensemble(b,np.transpose(a,(1,2,0)))\n",
    "                    rmes_score=rmse(a,b)\n",
    "                    mae_score=mae(a,b)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                    #crps_score_vsdr.append(skil)\n",
    "                    mae_score_vsdr.append(mae_score)\n",
    "                    rmse_score_vsdr.append(rmes_score)\n",
    "        if not os.path.exists(\"../save/rmse/bi_217/\"+str(year)):\n",
    "            dpt.mkdir(\"../save/rmse/bi_217/\"+str(year))\n",
    "            \n",
    "        if not os.path.exists(\"../save/mae/bi_217/\"+str(year)):\n",
    "            dpt.mkdir(\"../save/mae/bi_217/\"+str(year))\n",
    "            \n",
    "        if not os.path.exists(\"../save/crps/bi_217/\"+str(year)):\n",
    "            dpt.mkdir(\"../save/crps/bi_217/\"+str(year))\n",
    "#         np.save(\"../save/crps/bi_217/2010/lead_time_\"+str(lead),crps_score_vsdr)\n",
    "        np.save(\"../save/rmse/bi_217/\"+str(year)+\"/lead_time_\"+str(lead),rmse_score_vsdr)\n",
    "        np.save(\"../save/mae/bi_217/\"+str(year)+\"/lead_time_\"+str(lead),mae_score_vsdr)\n",
    "        \n",
    "        print(str(lead)+\" : \"+str(np.array(rmse_score_vsdr).mean()),\",\")\n",
    "        print(str(lead)+\" : \"+str(np.array(mae_score_vsdr).mean()),\",\")\n",
    "            \n",
    "\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     main(year=1997)\n",
    "#     print(1)\n",
    "#     main(year=2010)    \n",
    "#     print(2)\n",
    "#     main(year=2012)    \n",
    "#     print(3)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T07:05:05.813858Z",
     "start_time": "2021-05-19T07:05:05.808256Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_hr_size_of_cali(fct_ens,obs):\n",
    "\n",
    "    score =0\n",
    "    mapp=np.load(\"mmap.npy\")\n",
    "#     assert np.isnan(fct_ens).any() and not np.isnan(obs).any(),\"data contains nan\"\n",
    "    score_map=np.zeros((fct_ens.shape[0],obs.shape[1],obs.shape[2]))\n",
    "    print(mapp.shape)\n",
    "    for i in range(obs.shape[1]):\n",
    "        for j in range(obs.shape[2]):\n",
    "#             if (np.array(fct_ens[:,i,j],dtype=np.float32)>2000).any() or (mapp[i,j]==-1).any():\n",
    "#             if (np.array(fct_ens[:,i,j],dtype=np.float32)>2000).any(): \n",
    "            if (np.array(fct_ens[:,mapp[i,j,0],mapp[i,j,1]],dtype=np.int)>=-0.00001).all():\n",
    "                score_map[:,i,j]=fct_ens[:,mapp[i,j,0],mapp[i,j,1]].copy()\n",
    "            else:\n",
    "                score_map[:,i,j]=np.inf\n",
    "#             score+=crps(fct_ens[:,i,j],obs[i,j])\n",
    "    return score_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T07:05:07.085105Z",
     "start_time": "2021-05-19T07:05:06.272891Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316, 376, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11, 316, 376)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.random.rand(11,900,900)\n",
    "b=np.random.rand(11,316,376)\n",
    "get_hr_size_of_cali(a,b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
