{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-12T02:06:47.126264Z",
     "start_time": "2021-01-12T02:05:43.671037Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training statistics:\n",
      "  ------------------------------\n",
      "  trainning name  |  pr_dem\n",
      "  ------------------------------\n",
      "  num of channels |     1\n",
      "  ------------------------------\n",
      "  num of threads  |     0\n",
      "  ------------------------------\n",
      "  batch_size     |    44\n",
      "  ------------------------------\n",
      "  using cpu only |     0\n",
      "0 : 1.5698968 ,\n",
      "1 : 1.5351208 ,\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-72568b6f11aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;31m#0 : 1.569896615362574 ,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-72568b6f11aa>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;31m#                     skil=vectcrps_m(a,b)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m                     \u001b[0mskil\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrps_ensemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;31m#                     skil=ps.crps_ensemble(b,np.transpose(a,(1,2,0)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\properscoring\\_crps.py\u001b[0m in \u001b[0;36mcrps_ensemble\u001b[1;34m(observations, forecasts, weights, issorted, axis)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mforecasts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_crps_ensemble_core\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforecasts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\properscoring\\_crps.py\u001b[0m in \u001b[0;36m_crps_ensemble_vectorized\u001b[1;34m(observations, forecasts, weights)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0msuppress_warnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Mean of empty slice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m             score += -0.5 * np.nanmean(weights_matrix * abs(forecasts_diff),\n\u001b[1;32m--> 230\u001b[1;33m                                        axis=(-2, -1))\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mobservations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mforecasts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mnanmean\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\lib\\nanfunctions.py\u001b[0m in \u001b[0;36mnanmean\u001b[1;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[0;32m    946\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"If a is inexact, then out must be inexact\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m     \u001b[0mcnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m     \u001b[0mtot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[0mavg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_divide_by_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2241\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 2242\u001b[1;33m                           initial=initial, where=where)\n\u001b[0m\u001b[0;32m   2243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import util.data_processing_tool as dpt\n",
    "\n",
    "import numpy as np\n",
    "import properscoring as ps\n",
    "from datetime import timedelta, date, datetime\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from datetime import timedelta, date, datetime\n",
    "# import args_parameter as args\n",
    "import torch,torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import time\n",
    "import xarray as xr\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import matplotlib as plt\n",
    "import argparse\n",
    "import sys\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import platform\n",
    "from datetime import timedelta, date, datetime\n",
    "\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from math import log10\n",
    "import time\n",
    "# from PrepareData import ACCESS_BARRA_crps\n",
    "import tqdm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "# ===========================================================\n",
    "# Training settings\n",
    "# ===========================================================\n",
    "\n",
    "\n",
    "\n",
    "class ACCESS_BARRA_crps(Dataset):\n",
    "    '''\n",
    "\n",
    "2.using my net to train one channel to one channel.\n",
    "   \n",
    "    '''\n",
    "    def __init__(self,start_date=date(1990, 1, 1),end_date=date(1990,12 , 31),regin=\"AUS\",lr_transform=None,hr_transform=None,shuffle=True,args=None):\n",
    "#         print(\"=> BARRA_R & ACCESS_S1 loading\")\n",
    "#         print(\"=> from \"+start_date.strftime(\"%Y/%m/%d\")+\" to \"+end_date.strftime(\"%Y/%m/%d\")+\"\")\n",
    "        self.file_BARRA_dir = args.file_BARRA_dir\n",
    "        self.file_ACCESS_dir = args.file_ACCESS_dir\n",
    "        self.args=args\n",
    "        \n",
    "        self.lr_transform = lr_transform\n",
    "        self.hr_transform = hr_transform\n",
    "\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        \n",
    "        self.regin = regin\n",
    "        self.leading_time_we_use=args.leading_time_we_use\n",
    "\n",
    "        self.ensemble_access=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "        self.ensemble=[]\n",
    "        for i in range(args.ensemble):\n",
    "            self.ensemble.append(self.ensemble_access[i])\n",
    "                \n",
    "        self.dates = self.date_range(start_date, end_date)\n",
    "        \n",
    "        \n",
    "        self.filename_list=self.get_filename_with_time_order(args.file_ACCESS_dir+\"pr/daily/\")\n",
    "#         if not os.path.exists(args.file_ACCESS_dir+\"pr/daily/\"):\n",
    "#             print(args.file_ACCESS_dir+\"pr/daily/\")\n",
    "#             print(\"no file or no permission\")\n",
    "        \n",
    "        \n",
    "        _,_,date_for_BARRA,time_leading=self.filename_list[0]\n",
    "        if shuffle:\n",
    "            random.shuffle(self.filename_list)\n",
    "        \n",
    "        \n",
    "#         if not os.path.exists(\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/1990/01/accum_prcp-fc-spec-PT1H-BARRA_R-v1-19900109T0600Z.sub.nc\"):\n",
    "#             print(self.file_BARRA_dir)\n",
    "#             print(\"no file or no permission!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        data_high=dpt.read_barra_data_fc_get_lat_lon(self.file_BARRA_dir,date_for_BARRA)\n",
    "        self.lat=data_high[1]\n",
    "        self.lon=data_high[1]\n",
    "#         self.shape=(316, 376)\n",
    "\n",
    "\n",
    "#         print(type(self.data_dem))\n",
    "        \n",
    "#             data_dem=dpt.add_lat_lon( dpt.read_dem(args.file_DEM_dir+\"dem-9s1.tif\"))\n",
    "#             self.dem_data=dpt.interp_tensor_2d(dpt.map_aust_old(data_dem,xrarray=False) ,self.shape )\n",
    "        \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filename_list)\n",
    "    \n",
    "\n",
    "    def date_range(self,start_date, end_date):\n",
    "        \"\"\"This function takes a start date and an end date as datetime date objects.\n",
    "        It returns a list of dates for each date in order starting at the first date and ending with the last date\"\"\"\n",
    "        return [start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "    \n",
    "    def get_filename_with_no_time_order(self,rootdir):\n",
    "        '''get filename first and generate label '''\n",
    "        _files = []\n",
    "        list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "        for i in range(0,len(list)):\n",
    "            path = os.path.join(rootdir,list[i])\n",
    "            if os.path.isdir(path):\n",
    "                _files.extend(self.get_filename_with_no_time_order(path))\n",
    "            if os.path.isfile(path):\n",
    "                if path[-3:]==\".nc\":\n",
    "                    _files.append(path)\n",
    "        return _files\n",
    "    \n",
    "    def get_filename_with_time_order(self,rootdir):\n",
    "        '''get filename first and generate label ,one different w'''\n",
    "        _files = []\n",
    "        for date in self.dates:\n",
    "            for i in range(self.leading_time_we_use,self.leading_time_we_use+1):\n",
    "            \n",
    "\n",
    "#                 filename=\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"cd\n",
    "#                 access_path=rootdir+en+\"/\"+\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "#                 print(access_path)\n",
    "                for en in self.ensemble:\n",
    "                    access_path=rootdir+en+\"/\"+\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "                    if os.path.exists(access_path):\n",
    "                        \n",
    "                    \n",
    "                        if date==self.end_date and i==1:\n",
    "                            break\n",
    "                        path=[]\n",
    "                        path.append(en)\n",
    "                        barra_date=date+timedelta(i)\n",
    "                        path.append(date)\n",
    "                        path.append(barra_date)\n",
    "                        path.append(i)\n",
    "                        _files.append(path)\n",
    "\n",
    "#     def get_filename_with_time_order(self,rootdir):\n",
    "#         '''get filename first and generate label ,one different w'''\n",
    "#         _files = []\n",
    "#         for date in self.dates:\n",
    "#             for en in self.ensemble:\n",
    "\n",
    "# #                 filename=\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"cd\n",
    "#                 access_path=rootdir+en+\"/\"+\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "# #                 print(access_path)\n",
    "#                 if os.path.exists(access_path):\n",
    "#                     for i in range(self.leading_time_we_use):\n",
    "#                         if date==self.end_date and i==1:\n",
    "#                             break\n",
    "#                         path=[]\n",
    "#                         path.append(en)\n",
    "#                         barra_date=date+timedelta(i)\n",
    "#                         path.append(date)\n",
    "#                         path.append(barra_date)\n",
    "#                         path.append(i)\n",
    "#                         _files.append(path)                        \n",
    "\n",
    "    #最后去掉第一行，然后shuffle\n",
    "        return _files\n",
    "\n",
    "    def mapping(self,X,min_val=0.,max_val=255.):\n",
    "        Xmin = np.min(X)\n",
    "        Xmax = np.max(X)\n",
    "        #将数据映射到[-1,1]区间 即a=-1，b=1\n",
    "        a = min_val\n",
    "        b = max_val\n",
    "        Y = a + (b-a)/(Xmax-Xmin)*(X-Xmin)\n",
    "        return Y\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        from filename idx get id\n",
    "        return lr,hr\n",
    "        '''\n",
    "        t=time.time()\n",
    "        \n",
    "        #read_data filemame[idx]\n",
    "        en,access_date,barra_date,time_leading=self.filename_list[idx]\n",
    "        \n",
    "\n",
    "#         lr=dpt.read_access_data(self.file_ACCESS_dir,en,date(2010,1,1),time_leading,\"pr\")[82:144,134:188]*86400 #nci\n",
    "        lr=dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"pr\")*86400\n",
    "\n",
    "        lr=dpt.interp_tensor_2d(lr,(79,94))\n",
    "        \n",
    "#         lr=np.expand_dims(lr,axis=2)\n",
    "\n",
    "#         lr=np.expand_dims(self.mapping(lr),axis=2)\n",
    "        label=dpt.read_barra_data_fc(self.file_BARRA_dir,barra_date)\n",
    "\n",
    "            \n",
    "#         if self.args.channels==1:\n",
    "#             lr=np.repeat(lr,3,axis=2)\n",
    "#         return self.lr_transform(Image.fromarray(lr)),self.lr_transform(Image.fromarray(self.data_dem)),self.hr_transform(Image.fromarray(label))\n",
    "\n",
    "        return self.lr_transform(Image.fromarray(lr)),torch.tensor(int(en[1:])),self.hr_transform(Image.fromarray(label)),torch.tensor(int(en[1:])),torch.tensor(int(access_date.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "\n",
    "    \n",
    "\n",
    "def crps(ensin,obs):\n",
    "    '''\n",
    "    @param ensin A vector of prediction\n",
    "    @param obs  A vector of observations\n",
    "    \n",
    "'''\n",
    "\n",
    "#     assert not np.isnan(ensin).any() and not np.isnan(obs).any(), \"data contains nan\"\n",
    "         \n",
    "    Fn = ECDF(ensin)\n",
    "    xn=np.sort(np.unique(ensin))\n",
    "    m=len(xn)\n",
    "    dn=np.diff(xn)\n",
    "    eq1=0\n",
    "    eq2=0\n",
    "    if(obs>xn[0] and obs<xn[m-1]): #obs在范围内\n",
    "        k=np.max(np.where(xn<=obs))#小于obs的最大值下标\n",
    "        x0 = xn[k] #小于obs的最大值\n",
    "        if k>0:\n",
    "            eq1=np.sum(Fn(xn[0:k+1])**2*np.append(dn[0:k], obs - xn[k]))#小于obs的所有值 的 百分比数 的平方\n",
    "        else:\n",
    "            eq1 =np.sum(Fn(xn[0])**2*(obs - xn[0]))\n",
    "        if k<m-2:\n",
    "\n",
    "            eq2=np.sum((1-Fn(xn[k:m-1]))**2*np.append(xn[k+1] - obs, dn[(k+1):(m-1)]))\n",
    "        else:\n",
    "            eq2 =np.sum((1-Fn(xn[m-2]))**2*(xn[m-1] - obs))\n",
    "\n",
    "    if obs <= xn[0]: # 观测值在之外\n",
    "        eq2 =np.sum(np.append(1, 1-Fn(xn[0:(m-1)]))**2*np.append(xn[0]-obs, dn))\n",
    "    if obs >= xn[m-1]:\n",
    "        eq1= np.sum(Fn(xn)**2*np.append(dn, obs - xn[m-1]))\n",
    "            \n",
    "    return eq1+eq2 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vectcrps_v(fct_ens,obs):\n",
    "    '''\n",
    "    #' @param fct_ens A 2D prediction\n",
    "    #' @param obs  A vector of observations\n",
    "    #' @return a crps vector'''\n",
    "    score =0\n",
    "\n",
    "    \n",
    "    fct_ens=fct_ens\n",
    "    assert not np.isnan(fct_ens).any() and not np.isnan(obs).any(),\"data contains nan\"\n",
    "    for i in range(obs.shape[0]):\n",
    "#         print(fct_ens[:,i],obs[i])\n",
    "        score+=crps(fct_ens[:,i],obs[i])\n",
    "  \n",
    "    return score\n",
    "\n",
    "\n",
    "def vectcrps_m(fct_ens,obs):\n",
    "    '''\n",
    "    #' @param fct_ens A 2D prediction 11*1*1\n",
    "    #' @param obs  A vector of observations\n",
    "    #' @return a crps vector'''\n",
    "    score =0\n",
    "#     assert np.isnan(fct_ens).any() and np.isnan(obs).any(),\"data contains nan\"\n",
    "    score_map=np.zeros((obs.shape[0],obs.shape[1]))\n",
    "    for i in range(obs.shape[0]):\n",
    "        for j in range(obs.shape[1]):\n",
    "            score_map[i,j]=crps(fct_ens[:,i,j],obs[i,j])\n",
    "#             score+=crps(fct_ens[:,i,j],obs[i,j])\n",
    "    return score_map\n",
    "    return score/(obs.shape[0]*obs.shape[1])   \n",
    "\n",
    "\n",
    "def vectcrps_cali(fct_ens,obs):\n",
    "\n",
    "    score =0\n",
    "    mapp=np.load(\"mmap.npy\")\n",
    "#     assert np.isnan(fct_ens).any() and not np.isnan(obs).any(),\"data contains nan\"\n",
    "    score_map=np.zeros((obs.shape[0],obs.shape[1]))\n",
    "    for i in range(obs.shape[0]):\n",
    "        for j in range(obs.shape[1]):\n",
    "#             if (np.array(fct_ens[:,i,j],dtype=np.float32)>2000).any() or (mapp[i,j]==-1).any():\n",
    "#             if (np.array(fct_ens[:,i,j],dtype=np.float32)>2000).any(): \n",
    "            if (np.array(fct_ens[:,mapp[i,j,0],mapp[i,j,1]],dtype=np.int)>=-0.00001).all():\n",
    "                score_map[i,j]=crps(fct_ens[:,mapp[i,j,0],mapp[i,j,1]],obs[i,j])\n",
    "            else:\n",
    "                score_map[i,j]=np.inf\n",
    "#             score+=crps(fct_ens[:,i,j],obs[i,j])\n",
    "    return score_map\n",
    "\n",
    "\n",
    "def write_log(log,args):\n",
    "    print(log)\n",
    "    if not os.path.exists(\"./save/\"+args.train_name+\"/\"):\n",
    "        os.mkdir(\"./save/\"+args.train_name+\"/\")\n",
    "    my_log_file=open(\"./save/\"+args.train_name + '/train.txt', 'a')\n",
    "#     log=\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time())\n",
    "    my_log_file.write(log + '\\n')\n",
    "    my_log_file.close()\n",
    "    return\n",
    " \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Super Res Example')\n",
    "    # Hardware specifications\n",
    "    parser.add_argument('--n_threads', type=int, default=0,\n",
    "                        help='number of threads for data loading')\n",
    "\n",
    "    parser.add_argument('--cpu', action='store_true',help='cpu only?') \n",
    "\n",
    "    # hyper-parameters\n",
    "    parser.add_argument('--train_name', type=str, default=\"cali_crps\", help='training name')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=44, help='training batch size')\n",
    "    parser.add_argument('--testBatchSize', type=int, default=4, help='testing batch size')\n",
    "    parser.add_argument('--nEpochs', type=int, default=200, help='number of epochs to train for')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001, help='Learning Rate. Default=0.01')\n",
    "    parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\n",
    "\n",
    "    # model configuration\n",
    "    parser.add_argument('--upscale_factor', '-uf',  type=int, default=4, help=\"super resolution upscale factor\")\n",
    "    parser.add_argument('--model', '-m', type=str, default='vdsr', help='choose which model is going to use')\n",
    "\n",
    "    #data\n",
    "    parser.add_argument('--pr', type=bool, default=True,help='add-on pr?')\n",
    "\n",
    "    parser.add_argument('--train_start_time', type=type(datetime(1990,1,25)), default=datetime(1990,1,2),help='r?')\n",
    "    parser.add_argument('--train_end_time', type=type(datetime(1990,1,25)), default=datetime(1990,2,9),help='?')\n",
    "    parser.add_argument('--test_start_time', type=type(datetime(2012,1,1)), default=datetime(2012,1,1),help='a?')\n",
    "    \n",
    "    parser.add_argument('--test_end_time', type=type(datetime(2012,12,31)), default=datetime(2012,12,31),help='')\n",
    "\n",
    "    parser.add_argument('--dem', action='store_true',help='add-on dem?') \n",
    "    parser.add_argument('--psl', action='store_true',help='add-on psl?') \n",
    "    parser.add_argument('--zg', action='store_true',help='add-on zg?') \n",
    "    parser.add_argument('--tasmax', action='store_true',help='add-on tasmax?') \n",
    "    parser.add_argument('--tasmin', action='store_true',help='add-on tasmin?')\n",
    "    parser.add_argument('--leading_time_we_use', type=int,default=1\n",
    "                        ,help='add-on tasmin?')\n",
    "    parser.add_argument('--ensemble', type=int, default=11,help='total ensambles is 11') \n",
    "    parser.add_argument('--channels', type=float, default=0,help='channel of data_input must') \n",
    "    #[111.85, 155.875, -44.35, -9.975]\n",
    "    parser.add_argument('--domain', type=list, default=[112.9, 154.25, -43.7425, -9.0],help='dataset directory')\n",
    "\n",
    "    parser.add_argument('--file_ACCESS_dir', type=str, default=\"../../Data/\",help='dataset directory')\n",
    "    parser.add_argument('--file_BARRA_dir', type=str, default=\"../../Data/barra_aus/\",help='dataset directory')\n",
    "    parser.add_argument('--file_DEM_dir', type=str, default=\"../DEM/\",help='dataset directory')\n",
    "    parser.add_argument('--precision', type=str, default='single',choices=('single', 'half','double'),help='FP precision for test (single | half)')\n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    \n",
    "    # def main():\n",
    "\n",
    "#     init_date=date(1970, 1, 1)\n",
    "#     start_date=date(1990, 1, 2)\n",
    "#     end_date=date(2011,12,25)\n",
    "    sys = platform.system()\n",
    "    args.dem=False\n",
    "    args.train_name=\"pr_dem\"\n",
    "    args.channels=0\n",
    "    if args.pr:\n",
    "        args.channels+=1\n",
    "    if args.zg:\n",
    "        args.channels+=1\n",
    "    if args.psl:\n",
    "        args.channels+=1\n",
    "    if args.tasmax:\n",
    "        args.channels+=1\n",
    "    if args.tasmin:\n",
    "        args.channels+=1\n",
    "    if args.dem:\n",
    "        args.channels+=1\n",
    "    print(\"training statistics:\")\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  trainning name  |  %s\"%args.train_name)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of channels | %5d\"%args.channels)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  batch_size     | %5d\"%args.batch_size)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  using cpu only | %5d\"%args.cpu)\n",
    "\n",
    "    lr_transforms = transforms.Compose([\n",
    "        transforms.Resize((316, 376)),\n",
    "    #     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(30),\n",
    "        transforms.ToTensor()\n",
    "    #     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "\n",
    "    hr_transforms = transforms.Compose([\n",
    "    #         transforms.Resize((316, 376)),\n",
    "    #     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(30),\n",
    "        transforms.ToTensor()\n",
    "    #     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "\n",
    "    data_set=ACCESS_BARRA_crps(args.test_start_time,args.test_end_time,lr_transform=lr_transforms,hr_transform=hr_transforms,shuffle=False,args=args)\n",
    "\n",
    "\n",
    "\n",
    "    #     #######################################################################\n",
    "\n",
    "    test_data=DataLoader(data_set,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=False,\n",
    "                                num_workers=args.n_threads,drop_last=True)\n",
    "\n",
    "    #     #######################################################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # net=torch.load(\"./save/vdsr_pr/best_test.pth\")\n",
    "    # net=torch.load(\"../data/model/vdsr_pr/best_test.pth\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     ##############################################\n",
    "#     write_log(\"start\",args)\n",
    "    #     max_error=np.inf\n",
    "    #     val_max_error=np.inf\n",
    "\n",
    "    #     print(data_set.filename_list)\n",
    "\n",
    "    # for e in range(args.nEpochs):\n",
    "    #         loss=0\n",
    "    for lead in range(30):\n",
    "        args.leading_time_we_use=lead\n",
    "\n",
    "        data_set=ACCESS_BARRA_crps(args.test_start_time,args.test_end_time,lr_transform=lr_transforms,hr_transform=hr_transforms,shuffle=False,args=args)\n",
    "\n",
    "\n",
    "        test_data=DataLoader(data_set,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                shuffle=False,\n",
    "                                    num_workers=args.n_threads,drop_last=False)\n",
    "\n",
    "\n",
    "        crps_score_vsdr=[]\n",
    "        start=time.time()\n",
    "        fmt = '%Y%m%d'\n",
    "#         print(dpt.read_access_data('../../Data/','e01',date(2010,1,1),0,\"pr\"))\n",
    "\n",
    "    #         test_data=tqdm.tqdm(test_data)\n",
    "        for batch, (pr,dem,hr,en,data_time,idx) in enumerate(test_data):\n",
    "\n",
    "            with torch.set_grad_enabled(False):\n",
    "\n",
    "    #                 sr = net(pr)\n",
    "                sr_np=pr.cpu().numpy()\n",
    "                hr_np=hr.cpu().numpy()\n",
    "#                 print(pr.shape)\n",
    "#                 print(hr.shape)\n",
    "#                 print(en)\n",
    "#                 print(data_time)\n",
    "                for i in range(args.batch_size//args.ensemble):\n",
    "                    a=np.squeeze( sr_np[i*args.ensemble:(i+1)*args.ensemble])\n",
    "                    b=np.squeeze(hr_np[i*args.ensemble])\n",
    "#                     print(a.shape,b.shape)\n",
    "#                     skil=vectcrps_m(a,b)\n",
    "                    \n",
    "                    skil=ps.crps_ensemble(b,np.transpose(a,(1,2,0)))\n",
    "\n",
    "#                     skil=ps.crps_ensemble(b,np.transpose(a,(1,2,0)))\n",
    "#                     print(skil.shape)\n",
    "#                     time_tuple = time.strptime(str(data_time[i*args.ensemble].item()), fmt)\n",
    "#                     year, month, day = time_tuple[:3]\n",
    "    #                 print(a[:,20])\n",
    "    #                 print(skil[skil<1000000].shape)\n",
    "    #                 a_date = date(year, month, dy)\n",
    "        #             np.save(\"../crps/vsdr_pr/\"+(a_date+timedelta(idx[i*args.ensemble].item())).strftime(\"%Y%m%d\")+'_50station',skil)\n",
    "                    crps_score_vsdr.append(skil)\n",
    "            \n",
    "        if not os.path.exists(\"../save/crps/bi_217/2010\"):\n",
    "            dpt.mkdir(\"../save/crps/bi_217/2010/\")\n",
    "        np.save(\"../save/crps/bi_217/2010/lead_time_\"+str(lead),crps_score_vsdr)\n",
    "        print(str(lead)+\" : \"+str(np.array(crps_score_vsdr).mean()),\",\")\n",
    "\n",
    "            \n",
    "\n",
    "main()    \n",
    "\n",
    "#0 : 1.569896615362574 ,\n",
    "#1 : 1.5351210985633477 ,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T06:18:13.122260Z",
     "start_time": "2021-01-11T06:18:13.116623Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0023745315"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(dpt.read_access_data('../../Data/','e01',date(2010,1,1),3,\"pr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T05:01:51.723521Z",
     "start_time": "2021-01-11T05:01:51.699562Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ens=np.random.rand(316, 376,20)\n",
    "obs=np.random.rand(316,376)\n",
    "####################\n",
    "ens=np.random.rand(11,317,376)\n",
    "obs=np.random.rand(317,376)\n",
    "transpose()\n",
    "# print(ens,obs)\n",
    "# ps.crps_ensemble(obs,ens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-11T05:10:32.109182Z",
     "start_time": "2021-01-11T05:10:32.091201Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/575 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(316, 376)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "read_access_data() missing 2 required positional arguments: 'date_time' and 'leading'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3e55964d127f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mensamble\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0men\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mensemble_access\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_access_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_ACCESS_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0men\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: read_access_data() missing 2 required positional arguments: 'date_time' and 'leading'"
     ]
    }
   ],
   "source": [
    "#计算climatology for validation \n",
    "\n",
    "# from scipy.stats import norm\n",
    "# ps.crps_ensemble(obs,ens).shape\n",
    "\n",
    "\n",
    "\n",
    "###################paramter you need to set###########################\n",
    "dates_needs=dpt.date_range(date(2010, 1, 1),date(2011, 7, 29))\n",
    "time_windows=1          #range of the target time as climatology ensemble\n",
    "file_BARRA_dir=\"../../Data/barra_aus/\"\n",
    "file_ACCESS_dir='../../Data/'\n",
    "######################################################################\n",
    "ensemble_access=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "date_map=np.array(dates_needs)\n",
    "# np.where(date_map==date(2012, 1, 1))\n",
    "crps_ref=[]\n",
    "import tqdm\n",
    "for target_date in tqdm.tqdm(dates_needs):\n",
    "    hr=dpt.read_barra_data_fc(file_BARRA_dir,target_date)\n",
    "    size=hr.shape\n",
    "    print(size)\n",
    "    ensamble=[]\n",
    "    for en in ensemble_access:\n",
    "        sr=dpt.read_access_data(file_ACCESS_dir,en)\n",
    "        sr=cv2.resize(sr, (size[1], size[0]),interpolation=cv2.INTER_CUBIC)\n",
    "        print(sr.shape)\n",
    "        ensamble.append(sr)\n",
    "        \n",
    "    \n",
    "\n",
    "    break\n",
    "    \n",
    "    \n",
    "#     ensamble=[]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#         filename=file_BARRA_dir+str(y)+target_date.strftime(\"%m%d\")+\".nc\"\n",
    "#         if os.path.exists(filename):\n",
    "#             t=date(y,target_date.month,target_date.day)\n",
    "#             sr=dpt.read_barra_data_fc(file_BARRA_dir,t)\n",
    "#             ensamble.append(sr)\n",
    "            \n",
    "#     if ensamble:\n",
    "#         ensamble=np.array(ensamble)\n",
    "#         print(ensamble.shape)\n",
    "#         a=ps.crps_ensemble(hr,ensamble.transpose(1,2,0))\n",
    "#         crps_ref.append(a)\n",
    "\n",
    "# if not os.path.exists('../save/crps/BI/'):\n",
    "#     dpt.mkdir('../save/crps/BI/')\n",
    "# np.save(\"../save/crps/BI/BI\"+str(2010)+\"_all_lead_time_windows_11\",np.array(crps_ref))\n",
    "# # print(\" : \"+str(np.array(crps_ref).mean()),\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
