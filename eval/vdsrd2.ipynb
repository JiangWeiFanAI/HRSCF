{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-19T06:27:28.563461Z",
     "start_time": "2021-05-19T06:27:28.447463Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_processing_tool'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-187fa688b229>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mdata_processing_tool\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdpt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtimedelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# import args_parameter as args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_processing_tool'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import data_processing_tool as dpt\n",
    "from datetime import timedelta, date, datetime\n",
    "# import args_parameter as args\n",
    "import torch,torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import time\n",
    "import xarray as xr\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import matplotlib as plt\n",
    "import argparse\n",
    "import sys\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import platform\n",
    "from datetime import timedelta, date, datetime\n",
    "from model import vdsrd2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from math import log10\n",
    "import time\n",
    "# from PrepareData import ACCESS_BARRA_crps\n",
    "import tqdm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "# ===========================================================\n",
    "# Training settings\n",
    "# ===========================================================\n",
    "\n",
    "\n",
    "\n",
    "class ACCESS_BARRA_crps(Dataset):\n",
    "    '''\n",
    "\n",
    "2.using my net to train one channel to one channel.\n",
    "   \n",
    "    '''\n",
    "    def __init__(self,start_date=date(1990, 1, 1),end_date=date(1990,12 , 31),regin=\"AUS\",lr_transform=None,hr_transform=None,shuffle=True,args=None):\n",
    "#         print(\"=> BARRA_R & ACCESS_S1 loading\")\n",
    "#         print(\"=> from \"+start_date.strftime(\"%Y/%m/%d\")+\" to \"+end_date.strftime(\"%Y/%m/%d\")+\"\")\n",
    "        self.file_BARRA_dir = args.file_BARRA_dir\n",
    "        self.file_ACCESS_dir = args.file_ACCESS_dir\n",
    "        self.args=args\n",
    "        \n",
    "        self.lr_transform = lr_transform\n",
    "        self.hr_transform = hr_transform\n",
    "\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        \n",
    "        self.regin = regin\n",
    "        self.leading_time_we_use=args.leading_time_we_use\n",
    "\n",
    "        self.ensemble_access=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "        self.ensemble=[]\n",
    "        for i in range(args.ensemble):\n",
    "            self.ensemble.append(self.ensemble_access[i])\n",
    "                \n",
    "        self.dates = self.date_range(start_date, end_date)\n",
    "        \n",
    "        \n",
    "        self.filename_list=self.get_filename_with_time_order(args.file_ACCESS_dir+\"pr/daily/\")\n",
    "#         if not os.path.exists(args.file_ACCESS_dir+\"pr/daily/\"):\n",
    "#             print(args.file_ACCESS_dir+\"pr/daily/\")\n",
    "#             print(\"no file or no permission\")\n",
    "        \n",
    "        \n",
    "        _,_,date_for_BARRA,time_leading=self.filename_list[0]\n",
    "        if shuffle:\n",
    "            random.shuffle(self.filename_list)\n",
    "        \n",
    "        \n",
    "#         if not os.path.exists(\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/1990/01/accum_prcp-fc-spec-PT1H-BARRA_R-v1-19900109T0600Z.sub.nc\"):\n",
    "#             print(self.file_BARRA_dir)\n",
    "#             print(\"no file or no permission!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        data_high=dpt.read_barra_data_fc_get_lat_lon(self.file_BARRA_dir,date_for_BARRA)\n",
    "        self.lat=data_high[1]\n",
    "        self.lon=data_high[1]\n",
    "#         self.shape=(316, 376)\n",
    "\n",
    "\n",
    "#         print(type(self.data_dem))\n",
    "        \n",
    "#             data_dem=dpt.add_lat_lon( dpt.read_dem(args.file_DEM_dir+\"dem-9s1.tif\"))\n",
    "#             self.dem_data=dpt.interp_tensor_2d(dpt.map_aust_old(data_dem,xrarray=False) ,self.shape )\n",
    "        \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.filename_list)\n",
    "    \n",
    "\n",
    "    def date_range(self,start_date, end_date):\n",
    "        \"\"\"This function takes a start date and an end date as datetime date objects.\n",
    "        It returns a list of dates for each date in order starting at the first date and ending with the last date\"\"\"\n",
    "        return [start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "    \n",
    "    def get_filename_with_no_time_order(self,rootdir):\n",
    "        '''get filename first and generate label '''\n",
    "        _files = []\n",
    "        list = os.listdir(rootdir) #åˆ—å‡ºæ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰çš„ç›®å½•ä¸Žæ–‡ä»¶\n",
    "        for i in range(0,len(list)):\n",
    "            path = os.path.join(rootdir,list[i])\n",
    "            if os.path.isdir(path):\n",
    "                _files.extend(self.get_filename_with_no_time_order(path))\n",
    "            if os.path.isfile(path):\n",
    "                if path[-3:]==\".nc\":\n",
    "                    _files.append(path)\n",
    "        return _files\n",
    "    \n",
    "    def get_filename_with_time_order(self,rootdir):\n",
    "        '''get filename first and generate label ,one different w'''\n",
    "        _files = []\n",
    "        for date in self.dates:\n",
    "            for i in range(self.leading_time_we_use,self.leading_time_we_use+1):\n",
    "            \n",
    "\n",
    "#                 filename=\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"cd\n",
    "#                 access_path=rootdir+en+\"/\"+\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "#                 print(access_path)\n",
    "                for en in self.ensemble:\n",
    "                    access_path=rootdir+en+\"/\"+\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "                    if os.path.exists(access_path):\n",
    "                        \n",
    "                    \n",
    "                        if date==self.end_date and i==1:\n",
    "                            break\n",
    "                        path=[]\n",
    "                        path.append(en)\n",
    "                        barra_date=date+timedelta(i)\n",
    "                        path.append(date)\n",
    "                        path.append(barra_date)\n",
    "                        path.append(i)\n",
    "                        _files.append(path)\n",
    "\n",
    "#     def get_filename_with_time_order(self,rootdir):\n",
    "#         '''get filename first and generate label ,one different w'''\n",
    "#         _files = []\n",
    "#         for date in self.dates:\n",
    "#             for en in self.ensemble:\n",
    "\n",
    "# #                 filename=\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"cd\n",
    "#                 access_path=rootdir+en+\"/\"+\"da_pr_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "# #                 print(access_path)\n",
    "#                 if os.path.exists(access_path):\n",
    "#                     for i in range(self.leading_time_we_use):\n",
    "#                         if date==self.end_date and i==1:\n",
    "#                             break\n",
    "#                         path=[]\n",
    "#                         path.append(en)\n",
    "#                         barra_date=date+timedelta(i)\n",
    "#                         path.append(date)\n",
    "#                         path.append(barra_date)\n",
    "#                         path.append(i)\n",
    "#                         _files.append(path)                        \n",
    "\n",
    "    #æœ€åŽåŽ»æŽ‰ç¬¬ä¸€è¡Œï¼Œç„¶åŽshuffle\n",
    "        return _files\n",
    "\n",
    "    def mapping(self,X,min_val=0.,max_val=255.):\n",
    "        Xmin = np.min(X)\n",
    "        Xmax = np.max(X)\n",
    "        #å°†æ•°æ®æ˜ å°„åˆ°[-1,1]åŒºé—´ å³a=-1ï¼Œb=1\n",
    "        a = min_val\n",
    "        b = max_val\n",
    "        Y = a + (b-a)/(Xmax-Xmin)*(X-Xmin)\n",
    "        return Y\n",
    "    def to01(self,lr):\n",
    "        return (lr-np.min(lr))/(np.max(lr)-np.min(lr))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        from filename idx get id\n",
    "        return lr,hr\n",
    "        '''\n",
    "        t=time.time()\n",
    "#         data[var_name][:7,82:144,134:188]*86400\n",
    "        #read_data filemame[idx]\n",
    "        en,access_date,barra_date,time_leading=self.filename_list[idx]\n",
    "        \n",
    "\n",
    "        lr=dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"pr\")[82:144,134:188]*86400\n",
    "#         lr=np.expand_dims(lr,axis=2)\n",
    "        lr=dpt.interp_tensor_2d(lr,(79,94))\n",
    "#         lr=np.expand_dims(self.mapping(lr),axis=2)\n",
    "        label=dpt.read_barra_data_fc(self.file_BARRA_dir,barra_date)\n",
    "\n",
    "        lr_zg=dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"zg\")[-3,82:144,134:188]\n",
    "        #lr_zg=dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"psl\")[82:144,134:188]\n",
    "        lr_zg=self.to01(lr_zg)\n",
    "        lr_zg=lr_zg*np.max(lr)\n",
    "\n",
    "        if self.args.psl:\n",
    "            lr_psl=dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"psl\")\n",
    "\n",
    "        if self.args.tasmax:\n",
    "            lr_tasmax=np.expand_dims(dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"tasmax\"),axis=2)\n",
    "\n",
    "\n",
    "        if self.args.tasmin:\n",
    "            lr_tasmin=dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"tasmin\")\n",
    "            \n",
    "#         if self.args.channels==1:\n",
    "#             lr=np.repeat(lr,3,axis=2)\n",
    "#         return self.lr_transform(Image.fromarray(lr)),self.lr_transform(Image.fromarray(self.data_dem)),self.hr_transform(Image.fromarray(label))\n",
    "\n",
    "        return self.lr_transform(Image.fromarray(lr)),self.lr_transform(Image.fromarray(lr_zg)),self.hr_transform(Image.fromarray(label)),torch.tensor(int(en[1:])),torch.tensor(int(access_date.strftime(\"%Y%m%d\"))),torch.tensor(time_leading)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def rmse(ens,hr):\n",
    "    '''\n",
    "    ens:(ensemble,H,W)\n",
    "    hr: (H,W)\n",
    "    '''\n",
    "    return np.sqrt((ens-hr).sum(axis=(0)))\n",
    "\n",
    "def mae(ens,hr):\n",
    "    '''\n",
    "    ens:(ensemble,H,W)\n",
    "    hr: (H,W)\n",
    "    '''\n",
    "    return np.abs((ens-hr)).sum(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def write_log(log,args):\n",
    "    print(log)\n",
    "    if not os.path.exists(\"./save/\"+args.train_name+\"/\"):\n",
    "        os.mkdir(\"./save/\"+args.train_name+\"/\")\n",
    "    my_log_file=open(\"./save/\"+args.train_name + '/train.txt', 'a')\n",
    "#     log=\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time())\n",
    "    my_log_file.write(log + '\\n')\n",
    "    my_log_file.close()\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def main(year):\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Super Res Example')\n",
    "    # Hardware specifications\n",
    "    parser.add_argument('--n_threads', type=int, default=0,\n",
    "                        help='number of threads for data loading')\n",
    "\n",
    "    parser.add_argument('--cpu', action='store_true',help='cpu only?') \n",
    "\n",
    "    # hyper-parameters\n",
    "    parser.add_argument('--train_name', type=str, default=\"cali_crps\", help='training name')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=44, help='training batch size')\n",
    "    parser.add_argument('--testBatchSize', type=int, default=4, help='testing batch size')\n",
    "    parser.add_argument('--nEpochs', type=int, default=200, help='number of epochs to train for')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001, help='Learning Rate. Default=0.01')\n",
    "    parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\n",
    "\n",
    "    # model configuration\n",
    "    parser.add_argument('--upscale_factor', '-uf',  type=int, default=4, help=\"super resolution upscale factor\")\n",
    "    parser.add_argument('--model', '-m', type=str, default='vdsr', help='choose which model is going to use')\n",
    "\n",
    "    #data\n",
    "    parser.add_argument('--pr', type=bool, default=True,help='add-on pr?')\n",
    "\n",
    "    parser.add_argument('--train_start_time', type=type(datetime(1990,1,25)), default=datetime(1990,1,2),help='r?')\n",
    "    parser.add_argument('--train_end_time', type=type(datetime(1990,1,25)), default=datetime(1990,2,9),help='?')\n",
    "    parser.add_argument('--test_start_time', type=type(datetime(1997,1,1)), default=datetime(2010,1,1),help='a?')\n",
    "    parser.add_argument('--test_end_time', type=type(datetime(1997,12,31)), default=datetime(2010,12,31),help='')\n",
    "\n",
    "    parser.add_argument('--dem', action='store_true',help='add-on dem?') \n",
    "    parser.add_argument('--psl', action='store_true',help='add-on psl?') \n",
    "    parser.add_argument('--zg', action='store_true',help='add-on zg?') \n",
    "    parser.add_argument('--tasmax', action='store_true',help='add-on tasmax?') \n",
    "    parser.add_argument('--tasmin', action='store_true',help='add-on tasmin?')\n",
    "    parser.add_argument('--leading_time_we_use', type=int,default=1\n",
    "                        ,help='add-on tasmin?')\n",
    "    parser.add_argument('--ensemble', type=int, default=11,help='total ensambles is 11') \n",
    "    parser.add_argument('--channels', type=float, default=0,help='channel of data_input must') \n",
    "    #[111.85, 155.875, -44.35, -9.975]\n",
    "    parser.add_argument('--domain', type=list, default=[112.9, 154.25, -43.7425, -9.0],help='dataset directory')\n",
    "\n",
    "    parser.add_argument('--file_ACCESS_dir', type=str, default=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\",help='dataset directory')\n",
    "\n",
    "    parser.add_argument('--file_BARRA_dir', type=str, default=\"../../Data/barra_aus/\",help='dataset directory')\n",
    "    parser.add_argument('--file_DEM_dir', type=str, default=\"../DEM/\",help='dataset directory')\n",
    "    parser.add_argument('--precision', type=str, default='single',choices=('single', 'half','double'),help='FP precision for test (single | half)')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # def main():\n",
    "\n",
    "#     init_date=date(1970, 1, 1)\n",
    "#     start_date=date(1990, 1, 2)\n",
    "#     end_date=date(2011,12,25)\n",
    "    sys = platform.system()\n",
    "    args.dem=False\n",
    "    args.train_name=\"pr_dem\"\n",
    "    args.channels=0\n",
    "    if args.pr:\n",
    "        args.channels+=1\n",
    "    if args.zg:\n",
    "        args.channels+=1\n",
    "    if args.psl:\n",
    "        args.channels+=1\n",
    "    if args.tasmax:\n",
    "        args.channels+=1\n",
    "    if args.tasmin:\n",
    "        args.channels+=1\n",
    "    if args.dem:\n",
    "        args.channels+=1\n",
    "    print(\"training statistics:\")\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  trainning name  |  %s\"%args.train_name)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of channels | %5d\"%args.channels)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  batch_size     | %5d\"%args.batch_size)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  using cpu only | %5d\"%args.cpu)\n",
    "\n",
    "    lr_transforms = transforms.Compose([\n",
    "        transforms.Resize((316, 376)),\n",
    "    #     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(30),\n",
    "        transforms.ToTensor()\n",
    "    #     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "\n",
    "    hr_transforms = transforms.Compose([\n",
    "    #         transforms.Resize((316, 376)),\n",
    "    #     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(30),\n",
    "        transforms.ToTensor()\n",
    "    #     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "    args.test_start_time=datetime(year,1,1)\n",
    "    args.test_end_time=datetime(year,12,31)\n",
    "\n",
    "    data_set=ACCESS_BARRA_crps(args.test_start_time,args.test_end_time,lr_transform=lr_transforms,hr_transform=hr_transforms,shuffle=False,args=args)\n",
    "\n",
    "\n",
    "\n",
    "    #     #######################################################################\n",
    "\n",
    "    test_data=DataLoader(data_set,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=False,\n",
    "                                num_workers=args.n_threads,drop_last=True)\n",
    "\n",
    "    #     #######################################################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    net_state=torch.load(\"../save/vdsrd2/zg_val\"+str(year)+\"/best_test.pth\")['model'].module.state_dict()\n",
    "    net = vdsrd2()\n",
    "    net.load_state_dict(net_state)\n",
    "\n",
    "    #net=torch.load(\"./save/val10/best_test_45.pth\",map_location=device)\n",
    "    # net=torch.load(\"../data/model/vdsr_pr/best_test.pth\")\n",
    "    net.to(device)\n",
    "\n",
    "    def prepare( l, device=False):\n",
    "        def _prepare(tensor):\n",
    "            if args.precision == 'half': tensor = tensor.half()\n",
    "            if args.precision == 'single': tensor = tensor.float()\n",
    "            return tensor.to(device)\n",
    "\n",
    "        return [_prepare(_l) for _l in l]\n",
    "\n",
    "    #     ##############################################\n",
    "\n",
    "    for lead in range(217):\n",
    "        args.leading_time_we_use=lead\n",
    "\n",
    "        data_set=ACCESS_BARRA_crps(args.test_start_time,args.test_end_time,lr_transform=lr_transforms,hr_transform=hr_transforms,shuffle=False,args=args)\n",
    "\n",
    "\n",
    "        test_data=DataLoader(data_set,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                shuffle=False,\n",
    "                                    num_workers=args.n_threads,drop_last=False)\n",
    "\n",
    "\n",
    "        crps_score_vsdr=[]\n",
    "        mae_score_vsdr=[]\n",
    "        rmse_score_vsdr=[]\n",
    "\n",
    "        start=time.time()\n",
    "        fmt = '%Y%m%d'\n",
    "\n",
    "\n",
    "    #         test_data=tqdm.tqdm(test_data)\n",
    "        for batch, (pr,zg,hr,en,data_time,idx) in enumerate(test_data):\n",
    "\n",
    "            pr,zg,hr= prepare([pr,zg,hr],device)\n",
    "        #             print(en,data_time,idx)\n",
    "        #             optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(False):\n",
    "\n",
    "                sr = net(pr,zg)\n",
    "                sr_np=sr.cpu().numpy()\n",
    "                hr_np=hr.cpu().numpy()\n",
    "                for i in range(args.batch_size//args.ensemble):\n",
    "                    a=np.squeeze( sr_np[i*args.ensemble:(i+1)*args.ensemble])\n",
    "                    b=np.squeeze(hr_np[i*args.ensemble])\n",
    "                    \n",
    "#                     skil=vectcrps_m(a,b)                    \n",
    "                    rmes_score=rmse(a,b)\n",
    "                    mae_score=mae(a,b)\n",
    "\n",
    "\n",
    "#                     crps_score_vsdr.append(skil)\n",
    "                    mae_score_vsdr.append(mae_score)\n",
    "                    rmse_score_vsdr.append(rmes_score)\n",
    "        if not os.path.exists(\"../save/rmse/vdsrd2/\"+str(year)):\n",
    "            dpt.mkdir(\"../save/rmse/vdsrd2/\"+str(year))\n",
    "            \n",
    "        if not os.path.exists(\"../save/mae/vdsrd2/\"+str(year)):\n",
    "            dpt.mkdir(\"../save/mae/vdsrd2/\"+str(year))\n",
    "            \n",
    "        if not os.path.exists(\"../save/crps/vdsrd2/\"+str(year)):\n",
    "            dpt.mkdir(\"../save/crps/vdsrd2/\"+str(year))\n",
    "#         np.save(\"../save/crps/bi_217/2010/lead_time_\"+str(lead),crps_score_vsdr)\n",
    "        np.save(\"../save/rmse/vdsrd2/\"+str(year)+\"/lead_time_\"+str(lead),rmse_score_vsdr)\n",
    "        np.save(\"../save/mae/vdsrd2/\"+str(year)+\"/lead_time_\"+str(lead),mae_score_vsdr)\n",
    "        \n",
    "        print(str(lead)+\" : \"+str(np.array(rmse_score_vsdr).mean()),\",\")\n",
    "        print(str(lead)+\" : \"+str(np.array(mae_score_vsdr).mean()),\",\")\n",
    "            \n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main(year=1997) \n",
    "    print('1997done')\n",
    "    main(year=2010) \n",
    "    print('2010done')\n",
    "    main(year=2012)\n",
    "    print('2012done')\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
