{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T21:50:14.447649Z",
     "start_time": "2021-03-02T21:50:06.201498Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import util.PrepareData as pdata\n",
    "import util.data_processing_tool as dpt\n",
    "\n",
    "from datetime import date\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from datetime import timedelta, date, datetime\n",
    "\n",
    "import util.constant_param as consparam\n",
    "\n",
    "\n",
    "# station_5={\n",
    "#      'WAGGA WAGGA AMO': [-35.1583, 147.4575],\n",
    "#      'MENINGIE': [-35.6902, 139.3375],\n",
    "#      'SURAT': [-27.1591, 149.0702],\n",
    "#      'MILDURA AIRPORT': [-34.2358, 142.0867],\n",
    "#      'ORANGE AGRICULTURAL INSTITUTE': [-33.3211, 149.0828],\n",
    "# }\n",
    "\n",
    "# station_5_idx={\n",
    "# #     坐标\n",
    "#     'WAGGA WAGGA AMO': (98, 176),\n",
    "#      'MENINGIE': (97, 167),\n",
    "#      'SURAT': (113, 178),\n",
    "#      'MILDURA AIRPORT': (100, 170),\n",
    "#      'ORANGE AGRICULTURAL INSTITUTE': (102, 178)}\n",
    "\n",
    "# root_dir='../../data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T22:02:15.253095Z",
     "start_time": "2021-03-01T21:58:41.946173Z"
    }
   },
   "outputs": [],
   "source": [
    "start_date=date(1990, 1, 2)\n",
    "end_date=date(2012,12,25)\n",
    "dates=[start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "file_BARRA_dir=\"../../Data/barra_aus/\"\n",
    "\n",
    "def get_barra_cali_data():\n",
    "\n",
    "    for i in dates:\n",
    "        data_high=dpt.read_barra_data_fc(file_BARRA_dir,i)\n",
    "        data_50=[]\n",
    "        station_name=[]\n",
    "        for station in consparam.station_50_index_for_size_of_hr_sr_station_code.keys():\n",
    "#             print(station)\n",
    "            idx_i=consparam.station_50_index_for_size_of_hr_sr_station_code[station][0]\n",
    "            idx_j=consparam.station_50_index_for_size_of_hr_sr_station_code[station][1]\n",
    "            data_50.append(data_high[idx_i][idx_j])\n",
    "            station_name.append(station)\n",
    "#             data_50.append(data_high[station[0]][station[1]])\n",
    "\n",
    "        data_50=np.array(data_50)\n",
    "        station_name=np.array(station_name)\n",
    "        dpt.mkdir('../../Data/barra_aus/50_station/')\n",
    "        f_w = nc.Dataset('../../Data/barra_aus/50_station/'+i.strftime(\"%Y%m%d\")+'.nc','w',format = 'NETCDF4')\n",
    "        f_w.createDimension('station',50)\n",
    "\n",
    "        f_w.createVariable('station',np.int32,('station'))\n",
    "\n",
    "\n",
    "        f_w.variables['station'][:] = station_name\n",
    "#         f_w.variables['lon'][:] = label[\"lon\"].data\n",
    "\n",
    "        f_w.createVariable( 'barra', np.float32, ('station'))\n",
    "        f_w.variables['barra'][:] = data_50\n",
    "\n",
    "        f_w.close()\n",
    "\n",
    "#     print(dpt.read_barra_data_fc(\"../data/barra_aus/\",demdo_date)[78,315])\n",
    "\n",
    "#  'WAGGA WAGGA AMO': [78, 315],\n",
    "get_barra_cali_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T23:12:07.716001Z",
     "start_time": "2021-03-01T23:12:07.709988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Dataset'>\n",
       "root group (NETCDF4 data model, file format HDF5):\n",
       "    dimensions(sizes): station(50)\n",
       "    variables(dimensions): int32 station(station), float32 barra(station)\n",
       "    groups: "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc.Dataset('../../Data/barra_aus/50_station/19900102.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import util.PrepareData as pdata\n",
    "import util.data_processing_tool as dpt\n",
    "\n",
    "from datetime import date\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from datetime import timedelta, date, datetime\n",
    "\n",
    "import util.constant_param as consparam\n",
    "start_date=date(1990, 1, 2)\n",
    "end_date=date(2012,12,25)\n",
    "dates=[start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "#50 station\n",
    "def get_access_pr_data():# 顺便保存 BI数据\n",
    "    import netCDF4 as nc\n",
    "#     ensemble=['e01','e02']\n",
    "    sys = platform.system()\n",
    "    init_date=date(1970, 1, 1)\n",
    "    start_date=date(1990, 1, 1)\n",
    "    end_date=date(2012,12,25) #if 929 is true we should substract 1 day  \n",
    "    if sys == \"Windows\":\n",
    "        init_date=date(1970, 1, 1)\n",
    "        start_date=date(1990, 1, 1)\n",
    "        end_date=date(2012,12,25) #if 929 is true we should substract 1 day   \n",
    "        file_ACCESS_dir=\"H:/climate/access-s1/\" \n",
    "        file_BARRA_dir=\"D:/dataset/accum_prcp/\"\n",
    "    #         args.file_ACCESS_dir=\"E:/climate/access-s1/\"\n",
    "    #         args.file_BARRA_dir=\"C:/Users/JIA059/barra/\"\n",
    "        file_DEM_dir=\"../DEM/\"\n",
    "    else:\n",
    "        file_ACCESS_dir_pr=\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "        file_ACCESS_dir=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\"\n",
    "        # training_name=\"temp01\"\n",
    "        file_BARRA_dir=\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/\"\n",
    "\n",
    "        \n",
    "    nine2nine=True\n",
    "    date_minus_one=1\n",
    "    leading_time_we_use=7\n",
    "    ensemble=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "\n",
    "    var_name=\"pr\"\n",
    "    dates=[start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "    file_list=get_filename_with_time_order(file_ACCESS_dir+var_name+'/daily/',ensemble,dates,var_name)\n",
    "    time_leading=217\n",
    "\n",
    "    lat_name=\"lat\"\n",
    "    lon_name=\"lon\"\n",
    "\n",
    "#     print(file_list)\n",
    "    for i in file_list:\n",
    "#         print(i)\n",
    "        data = Dataset(i[0], 'r')\n",
    "        var = data[var_name][:,82:144,134:188]*86400\n",
    "        lat = data[lat_name][:][82:144]\n",
    "        lon = data[lon_name][:][134:188]\n",
    "#         print(var.shape)\n",
    "        data.close()\n",
    "    #         lr=dpt.read_access_data(i,idx=time_leading).data[82:144,134:188]*86400\n",
    "        result=np.zeros((217,316,376))\n",
    "        for idx,j in enumerate(var):\n",
    "            result[idx]=dpt.interp_tensor_2d( dpt.interp_tensor_2d(j,(79,94) ),(316, 376))\n",
    "\n",
    "            data_50=[]\n",
    "            station_name=[]\n",
    "            for station in consparam.station_50_index_for_size_of_hr_sr_station_code.keys():\n",
    "                idx_i=consparam.station_50_index_for_size_of_hr_sr_station_code[station][0]\n",
    "                idx_j=consparam.station_50_index_for_size_of_hr_sr_station_code[station][1]\n",
    "                data_50.append(result[:,idx_i,idx_j])\n",
    "                station_name.append(station)\n",
    "=\n",
    "\n",
    "        data_50=np.array(data_50)\n",
    "        station_name=np.array(station_name)\n",
    "        \n",
    "        \n",
    "        if not os.path.exists('../Data/'+var_name+'/daily_BI_50station/'+i[1]):\n",
    "            os.makedirs('../Data/'+var_name+'/daily_BI_50station/'+i[1])\n",
    "        \n",
    "        f_w = nc.Dataset('../Data/'+var_name+'/daily_BI_50station/'+i[1]+\"/da_\"+var_name+\"_\"+i[2].strftime(\"%Y%m%d\")+\"_\"+i[1]+'.nc','w',format = 'NETCDF4')\n",
    "\n",
    "        f_w.createDimension('time',time_leading)\n",
    "        f_w.createDimension('station',50)\n",
    "\n",
    "        f_w.createVariable('station',np.int32,('station'))\n",
    "        f_w.createVariable('time',np.int,('time'))\n",
    "\n",
    "\n",
    "\n",
    "        f_w.variables['station'][:] = station_name\n",
    "        f_w.variables['time'][:] =  np.array(range(1,218))\n",
    "\n",
    "\n",
    "        f_w.createVariable( 'pr', np.float32, ('station','time'))\n",
    "        f_w.variables['pr'][:] = data_50\n",
    "\n",
    "        f_w.close()\n",
    "            \n",
    "\n",
    "    \n",
    "            \n",
    "\n",
    "    \n",
    "            \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-01T23:08:44.265653Z",
     "start_time": "2021-03-01T23:08:44.261664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
       "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
       "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
       "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
       "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
       "       170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
       "       183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
       "       209, 210, 211, 212, 213, 214, 215, 216, 217])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(range(1,218))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import util.PrepareData as pdata\n",
    "import util.data_processing_tool as dpt\n",
    "\n",
    "from datetime import date\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from datetime import timedelta, date, datetime\n",
    "import platform \n",
    "import util.constant_param as consparam\n",
    "#50 station\n",
    "import os\n",
    "\n",
    "\n",
    "def get_filename_with_no_time_order(rootdir):\n",
    "    '''get filename first and generate label '''\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i],)\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(get_filename_with_no_time_order(path))\n",
    "        if os.path.isfile(path):\n",
    "            if path[-3:]==\".nc\":\n",
    "                _files.append(path)\n",
    "    return _files\n",
    "\n",
    "\n",
    "def get_filename_with_time_order(rootdir,ensemble,dates,var_name):\n",
    "    '''get filename first and generate label ,one different w'''\n",
    "    _files = []\n",
    "    for en in ensemble:\n",
    "        for date in dates:\n",
    "            access_path=rootdir+en+\"/\"+\"daq5_\"+var_name+\"_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "#             print(access_path)\n",
    "            if os.path.exists(access_path):\n",
    "#                 print(access_path)\n",
    "                path=[access_path]\n",
    "                path.append(en)\n",
    "                path.append(date)\n",
    "                _files.append(path)\n",
    "\n",
    "#最后去掉第一行，然后shuffle\n",
    "#     if nine2nine and date_minus_one==1:\n",
    "#         del _files[0]\n",
    "    return _files\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_access_cali_data():# 顺便保存 BI数据\n",
    "    import netCDF4 as nc\n",
    "#     ensemble=['e01','e02']\n",
    "    sys = platform.system()\n",
    "    init_date=date(1970, 1, 1)\n",
    "    start_date=date(1990, 1, 1)\n",
    "    end_date=date(2012,12,25) #if 929 is true we should substract 1 day  \n",
    "    if sys == \"Windows\":\n",
    "        init_date=date(1970, 1, 1)\n",
    "        start_date=date(1990, 1, 1)\n",
    "        end_date=date(2012,12,25) #if 929 is true we should substract 1 day   \n",
    "        file_ACCESS_dir=\"H:/climate/access-s1/\" \n",
    "        file_BARRA_dir=\"D:/dataset/accum_prcp/\"\n",
    "    #         args.file_ACCESS_dir=\"E:/climate/access-s1/\"\n",
    "    #         args.file_BARRA_dir=\"C:/Users/JIA059/barra/\"\n",
    "        file_DEM_dir=\"../DEM/\"\n",
    "    else:\n",
    "        file_ACCESS_dir_pr=\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "        file_ACCESS_dir = '/g/data/ub7/access-s1/hc/calibrated_5km_v3/atmos/pr/'\n",
    "        # training_name=\"temp01\"\n",
    "        file_BARRA_dir=\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/\"\n",
    "\n",
    "\n",
    "    ensemble=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "\n",
    "    var_name=\"pr\"\n",
    "    dates=[start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "    file_list=get_filename_with_time_order(file_ACCESS_dir+var_name+'/daily/',ensemble,dates,var_name)\n",
    "    time_leading=217\n",
    "\n",
    "    lat_name=\"lat\"\n",
    "    lon_name=\"lon\"\n",
    "\n",
    "#     print(file_list)\n",
    "    for i in file_list:\n",
    "#         print(i)\n",
    "        data = Dataset(i[0], 'r')\n",
    "        var = data[var_name][:]\n",
    "        lat = data[lat_name][:]\n",
    "        lon = data[lon_name][:]\n",
    "#         print(var.shape)\n",
    "        data.close()\n",
    "\n",
    "            \n",
    "        data_50=[]\n",
    "        station_name=[]\n",
    "        for station in consparam.station_50_index_for_size_of_calibration_station_code.keys():\n",
    "            idx_i=consparam.station_50_index_for_size_of_calibration_station_code[station][0]\n",
    "            idx_j=consparam.station_50_index_for_size_of_calibration_station_code[station][1]\n",
    "            data_50.append(var[:,idx_i,idx_j])\n",
    "            station_name.append(station)\n",
    "\n",
    "        data_50=np.array(data_50)\n",
    "        station_name=np.array(station_name)\n",
    "        \n",
    "        \n",
    "        if not os.path.exists('../Data/'+var_name+'/daily_BI_50station/'+i[1]):\n",
    "            os.makedirs('../Data/'+var_name+'/calibrated_5km_v3_50station/'+i[1])\n",
    "        \n",
    "        f_w = nc.Dataset('../Data/'+var_name+'/calibrated_5km_v3_50station/'+i[1]+\"/daq5_\"+var_name+\"_\"+i[2].strftime(\"%Y%m%d\")+\"_\"+i[1]+'.nc','w',format = 'NETCDF4')\n",
    "\n",
    "        f_w.createDimension('time',time_leading)\n",
    "        f_w.createDimension('station',50)\n",
    "\n",
    "        f_w.createVariable('station',np.int32,('station'))\n",
    "        f_w.createVariable('time',np.int,('time'))\n",
    "\n",
    "\n",
    "\n",
    "        f_w.variables['station'][:] = station_name\n",
    "        f_w.variables['time'][:] =  np.array(range(1,218))\n",
    "\n",
    "\n",
    "        f_w.createVariable( 'pr', np.float32, ('station','time'))\n",
    "        f_w.variables['pr'][:] = data_50\n",
    "        print(data_50.shape)\n",
    "        f_w.close()\n",
    "            \n",
    "\n",
    "    \n",
    "            \n",
    "\n",
    "    \n",
    "            \n",
    "if __name__=='__main__':\n",
    "    get_access_cali_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T00:51:25.292510Z",
     "start_time": "2021-03-02T00:51:25.285559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'netCDF4._netCDF4.Variable'>\n",
       "float32 pr(time, lat, lon)\n",
       "    long_name: precipition\n",
       "    cell_methods: time0: mean\n",
       "    units: mm/day\n",
       "    _FillValue: 1e+20\n",
       "    missing_value: 1e+20\n",
       "unlimited dimensions: time\n",
       "current shape = (217, 691, 886)\n",
       "filling on"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import util.PrepareData as pdata\n",
    "import util.data_processing_tool as dpt\n",
    "\n",
    "from datetime import date\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from datetime import timedelta, date, datetime\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from PIL import Image\n",
    "import util.constant_param as consparam\n",
    "start_date=date(1990, 1, 2)\n",
    "end_date=date(2012,12,25)\n",
    "dates=[start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "#50 station\n",
    "\n",
    "def get_filename_with_no_time_order(rootdir):\n",
    "    '''get filename first and generate label '''\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i],)\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(get_filename_with_no_time_order(path))\n",
    "        if os.path.isfile(path):\n",
    "            if path[-3:]==\".nc\":\n",
    "                _files.append(path)\n",
    "    return _files\n",
    "\n",
    "\n",
    "def get_filename_with_time_order(rootdir,ensemble,dates,var_name):\n",
    "    '''get filename first and generate label ,one different w'''\n",
    "    _files = []\n",
    "    for en in ensemble:\n",
    "        for date in dates:\n",
    "            access_path=rootdir+en+\"/\"+\"da_\"+var_name+\"_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "#             print(access_path)\n",
    "            if os.path.exists(access_path):\n",
    "#                 print(access_path)\n",
    "                path=[access_path]\n",
    "                path.append(en)\n",
    "                path.append(date)\n",
    "                _files.append(path)\n",
    "\n",
    "#最后去掉第一行，然后shuffle\n",
    "#     if nine2nine and date_minus_one==1:\n",
    "#         del _files[0]\n",
    "    return _files\n",
    "\n",
    "\n",
    "\n",
    "def get_access_SR_data():# 顺便保存 BI数据\n",
    "    \n",
    "    \n",
    "    lr_transforms = transforms.Compose([\n",
    "    transforms.Resize((316, 376)),\n",
    "#     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(30),\n",
    "    transforms.ToTensor()\n",
    "#     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    import netCDF4 as nc\n",
    "#     ensemble=['e01','e02']\n",
    "    sys = platform.system()\n",
    "    init_date=date(1970, 1, 1)\n",
    "    start_date=date(1990, 1, 1)\n",
    "    end_date=date(2012,12,25) #if 929 is true we should substract 1 day  \n",
    "    if sys == \"Windows\":\n",
    "        init_date=date(1970, 1, 1)\n",
    "        start_date=date(1990, 1, 1)\n",
    "        end_date=date(2012,12,25) #if 929 is true we should substract 1 day   \n",
    "        file_ACCESS_dir=\"H:/climate/access-s1/\" \n",
    "        file_BARRA_dir=\"D:/dataset/accum_prcp/\"\n",
    "    #         args.file_ACCESS_dir=\"E:/climate/access-s1/\"\n",
    "    #         args.file_BARRA_dir=\"C:/Users/JIA059/barra/\"\n",
    "        file_DEM_dir=\"../DEM/\"\n",
    "    else:\n",
    "        file_ACCESS_dir_pr=\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "        file_ACCESS_dir=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\"\n",
    "        # training_name=\"temp01\"\n",
    "        file_BARRA_dir=\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/\"\n",
    "\n",
    "        \n",
    "    ensemble=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "\n",
    "    var_name=\"pr\"\n",
    "    dates=[start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "    file_list=get_filename_with_time_order(file_ACCESS_dir+var_name+'/daily/',ensemble,dates,var_name)\n",
    "    time_leading=217\n",
    "\n",
    "    lat_name=\"lat\"\n",
    "    lon_name=\"lon\"\n",
    "\n",
    "#     print(file_list)\n",
    "    for i in file_list:\n",
    "#         print(i)\n",
    "        data = Dataset(i[0], 'r')\n",
    "        var = data[var_name][:,82:144,134:188]*86400\n",
    "        lat = data[lat_name][:][82:144]\n",
    "        lon = data[lon_name][:][134:188]\n",
    "#         print(var.shape)\n",
    "        data.close()\n",
    "    #         lr=dpt.read_access_data(i,idx=time_leading).data[82:144,134:188]*86400\n",
    "        result= torch.zeros((217,1,316,376),dtype=torch.float32)\n",
    "\n",
    "        for idx,j in enumerate(var):\n",
    "            if idx>= 217:\n",
    "                break\n",
    "            lr=dpt.interp_tensor_2d(j,(79,94))\n",
    "            result[idx]=lr_transforms(Image.fromarray(lr))\n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        net=torch.load('../save/model/vdsr_pr/best_test.pth',map_location=device)\n",
    "        def prepare( l, device=False):\n",
    "            def _prepare(tensor):\n",
    "                tensor = tensor.float()\n",
    "                return tensor.to(device)\n",
    "            return [_prepare(_l) for _l in l]\n",
    "        llrr=prepare([result],device)[0]\n",
    "        with torch.set_grad_enabled(False):\n",
    "            sr=net(llrr)\n",
    "        sr_np=np.squeeze(sr.cpu().numpy())\n",
    "        \n",
    "        data_50=[]\n",
    "        station_name=[]\n",
    "        for station in consparam.station_50_index_for_size_of_hr_sr_station_code.keys():\n",
    "            idx_i=consparam.station_50_index_for_size_of_hr_sr_station_code[station][0]\n",
    "            idx_j=consparam.station_50_index_for_size_of_hr_sr_station_code[station][1]\n",
    "            data_50.append(sr_np[:,idx_i,idx_j])\n",
    "            station_name.append(station)\n",
    "        data_50=np.array(data_50)\n",
    "        print(data_50.shape)\n",
    "        station_name=np.array(station_name)\n",
    "        \n",
    "        if not os.path.exists('../Data/'+var_name+'/daily_vdsrd_50station/'+i[1]):\n",
    "            os.makedirs('../Data/'+var_name+'/daily_vdsrd_50station/'+i[1])\n",
    "        \n",
    "        f_w = nc.Dataset('../Data/'+var_name+'/daily_vdsrd_50station/'+i[1]+\"/da_\"+var_name+\"_\"+i[2].strftime(\"%Y%m%d\")+\"_\"+i[1]+'.nc','w',format = 'NETCDF4')\n",
    "\n",
    "        f_w.createDimension('time',time_leading)\n",
    "        f_w.createDimension('station',50)\n",
    "\n",
    "        f_w.createVariable('station',np.int32,('station'))\n",
    "        f_w.createVariable('time',np.int,('time'))\n",
    "\n",
    "\n",
    "\n",
    "        f_w.variables['station'][:] = station_name\n",
    "        f_w.variables['time'][:] =  np.array(range(1,218))\n",
    "\n",
    "\n",
    "        f_w.createVariable( 'pr', np.float32, ('station','time'))\n",
    "        f_w.variables['pr'][:] = data_50\n",
    "\n",
    "        f_w.close()\n",
    "            \n",
    "\n",
    "    \n",
    "            \n",
    "\n",
    "    \n",
    "            \n",
    "if __name__=='__main__':\n",
    "    get_access_SR_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import util.PrepareData as pdata\n",
    "import util.data_processing_tool as dpt\n",
    "\n",
    "from datetime import date\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from datetime import timedelta, date, datetime\n",
    "import platform \n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import util.constant_param as consparam\n",
    "start_date=date(2012, 1, 2)\n",
    "end_date=date(2012,12,25)\n",
    "dates=[start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "from model import vdsrd2\n",
    "\n",
    "\n",
    "def get_filename_with_no_time_order(rootdir):\n",
    "    '''get filename first and generate label '''\n",
    "    _files = []\n",
    "    list = os.listdir(rootdir) #列出文件夹下所有的目录与文件\n",
    "    for i in range(0,len(list)):\n",
    "        path = os.path.join(rootdir,list[i],)\n",
    "        if os.path.isdir(path):\n",
    "            _files.extend(get_filename_with_no_time_order(path))\n",
    "        if os.path.isfile(path):\n",
    "            if path[-3:]==\".nc\":\n",
    "                _files.append(path)\n",
    "    return _files\n",
    "\n",
    "\n",
    "def get_filename_with_time_order(rootdir,ensemble,dates,var_name):\n",
    "    '''get filename first and generate label ,one different w'''\n",
    "    _files = []\n",
    "    for en in ensemble:\n",
    "        for date in dates:\n",
    "            access_path=rootdir+en+\"/\"+\"da_\"+var_name+\"_\"+date.strftime(\"%Y%m%d\")+\"_\"+en+\".nc\"\n",
    "#             print(access_path)\n",
    "            if os.path.exists(access_path):\n",
    "#                 print(access_path)\n",
    "                path=[access_path]\n",
    "                path.append(en)\n",
    "                path.append(date)\n",
    "                _files.append(path)\n",
    "\n",
    "#最后去掉第一行，然后shuffle\n",
    "#     if nine2nine and date_minus_one==1:\n",
    "#         del _files[0]\n",
    "    return _files\n",
    "\n",
    "def to01(lr):\n",
    "    return (lr-np.min(lr))/(np.max(lr)-np.min(lr))\n",
    "\n",
    "\n",
    "\n",
    "#50 station\n",
    "def get_access_SR_data():# 顺便保存 BI数据\n",
    "    \n",
    "    \n",
    "    lr_transforms = transforms.Compose([\n",
    "    transforms.Resize((316, 376)),\n",
    "#     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(30),\n",
    "    transforms.ToTensor()\n",
    "#     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    import netCDF4 as nc\n",
    "#     ensemble=['e01','e02']\n",
    "    sys = platform.system()\n",
    "    init_date=date(1970, 1, 1)\n",
    "    start_date=date(1990, 1, 1)\n",
    "    end_date=date(2012,12,25) #if 929 is true we should substract 1 day  \n",
    "    if sys == \"Windows\":\n",
    "        init_date=date(1970, 1, 1)\n",
    "        start_date=date(1990, 1, 1)\n",
    "        end_date=date(2012,12,25) #if 929 is true we should substract 1 day   \n",
    "        file_ACCESS_dir=\"H:/climate/access-s1/\" \n",
    "        file_BARRA_dir=\"D:/dataset/accum_prcp/\"\n",
    "    #         args.file_ACCESS_dir=\"E:/climate/access-s1/\"\n",
    "    #         args.file_BARRA_dir=\"C:/Users/JIA059/barra/\"\n",
    "        file_DEM_dir=\"../DEM/\"\n",
    "    else:\n",
    "        file_ACCESS_dir_pr=\"/g/data/ub7/access-s1/hc/raw_model/atmos/pr/daily/\"\n",
    "        file_ACCESS_dir=\"/g/data/ub7/access-s1/hc/raw_model/atmos/\"\n",
    "        # training_name=\"temp01\"\n",
    "        file_BARRA_dir=\"/g/data/ma05/BARRA_R/v1/forecast/spec/accum_prcp/\"\n",
    "\n",
    "        \n",
    "    ensemble=['e01','e02','e03','e04','e05','e06','e07','e08','e09','e10','e11']\n",
    "\n",
    "    var_name=\"pr\"\n",
    "    dates=[start_date + timedelta(x) for x in range((end_date - start_date).days + 1)]\n",
    "    file_list=get_filename_with_time_order(file_ACCESS_dir+var_name+'/daily/',ensemble,dates,var_name)\n",
    "    file_zg_list=get_filename_with_time_order(file_ACCESS_dir+var_name+'/daily/',ensemble,dates,'zg')\n",
    "\n",
    "#             lr_zg=dpt.read_access_data(self.file_ACCESS_dir,en,access_date,time_leading,\"zg\")[-3,82:144,134:188]\n",
    "\n",
    "    time_leading=217\n",
    "\n",
    "    lat_name=\"lat\"\n",
    "    lon_name=\"lon\"\n",
    "\n",
    "    print(file_zg_list)\n",
    "    for i,k in zip(file_list,file_zg_list):\n",
    "#         print(i)\n",
    "        data = Dataset(i[0], 'r')\n",
    "        var = data[var_name][:,82:144,134:188]*86400\n",
    "        \n",
    "        zg_var = data['zg'][-3,82:144,134:188]\n",
    "        lr_zg=to01(zg_var)\n",
    "        lr_zg=lr_zg*np.max(var)\n",
    "\n",
    "        lat = data[lat_name][:][82:144]\n",
    "        lon = data[lon_name][:][134:188]\n",
    "#         print(var.shape)\n",
    "        data.close()\n",
    "    #         lr=dpt.read_access_data(i,idx=time_leading).data[82:144,134:188]*86400\n",
    "        result= torch.zeros((217,1,316,376),dtype=torch.float32)\n",
    "\n",
    "        for idx,j in enumerate(var):\n",
    "            if idx>= 217:\n",
    "                break\n",
    "            lr=dpt.interp_tensor_2d(j,(79,94))\n",
    "            result[idx]=lr_transforms(Image.fromarray(lr))\n",
    "            \n",
    "        result_zg= torch.zeros((217,1,316,376),dtype=torch.float32)\n",
    "        for idx,j in enumerate(var_zg):\n",
    "            if idx>= 217:\n",
    "                break\n",
    "            result[idx]=lr_transforms(Image.fromarray(j))            \n",
    "\n",
    "\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        net_state=torch.load(\"../save/model/zg_val10/best_test_50.pth\")['model'].module.state_dict()\n",
    "        net = vdsr()\n",
    "        net.load_state_dict(net_state)\n",
    "\n",
    "#         net=torch.load('../save/model/vdsr_pr/best_test.pth',map_location=device)\n",
    "        def prepare( l, device=False):\n",
    "            def _prepare(tensor):\n",
    "                tensor = tensor.float()\n",
    "                return tensor.to(device)\n",
    "            return [_prepare(_l) for _l in l]\n",
    "        pr,zg=prepare([result,result_zg],device)\n",
    "\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            sr = net(pr,zg)\n",
    "        sr_np=np.squeeze(sr.cpu().numpy())\n",
    "        \n",
    "        data_50=[]\n",
    "        station_name=[]\n",
    "        for station in consparam.station_50_index_for_size_of_hr_sr_station_code.keys():\n",
    "            idx_i=consparam.station_50_index_for_size_of_hr_sr_station_code[station][0]\n",
    "            idx_j=consparam.station_50_index_for_size_of_hr_sr_station_code[station][1]\n",
    "            data_50.append(sr_np[:,idx_i,idx_j])\n",
    "            station_name.append(station)\n",
    "        data_50=np.array(data_50)\n",
    "        print(data_50.shape)\n",
    "        station_name=np.array(station_name)\n",
    "        \n",
    "        if not os.path.exists('../Data/'+var_name+'/daily_vdsrd2_50station/'+i[1]):\n",
    "            os.makedirs('../Data/'+var_name+'/daily_vdsrd2_50station/'+i[1])\n",
    "        \n",
    "        f_w = nc.Dataset('../Data/'+var_name+'/daily_vdsrd2_50station/'+i[1]+\"/da_\"+var_name+\"_\"+i[2].strftime(\"%Y%m%d\")+\"_\"+i[1]+'.nc','w',format = 'NETCDF4')\n",
    "\n",
    "        f_w.createDimension('time',time_leading)\n",
    "        f_w.createDimension('station',50)\n",
    "\n",
    "        f_w.createVariable('station',np.int32,('station'))\n",
    "        f_w.createVariable('time',np.int,('time'))\n",
    "\n",
    "\n",
    "\n",
    "        f_w.variables['station'][:] = station_name\n",
    "        f_w.variables['time'][:] =  np.array(range(1,218))\n",
    "\n",
    "\n",
    "        f_w.createVariable( 'pr', np.float32, ('station','time'))\n",
    "        f_w.variables['pr'][:] = data_50\n",
    "\n",
    "        f_w.close()\n",
    "            \n",
    "\n",
    "            \n",
    "if __name__=='__main__':\n",
    "    get_access_SR_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T00:13:10.623910Z",
     "start_time": "2021-03-03T00:13:00.393665Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\.libs\\libopenblas.TXA6YQSD3GCQQC22GEQ54J2UDCXDXHWN.gfortran-win_amd64.dll\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(217, 62, 54)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import netCDF4 as nc\n",
    "\n",
    "nc.Dataset('../../Data/da_zg_19900101_e01.nc')['zg'][:,-3,82:144,134:188].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T00:13:50.636680Z",
     "start_time": "2021-03-03T00:13:49.768884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(217, 62, 54)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nc.Dataset('../../Data/da_pr_19900109_e01.nc')['pr'][:,82:144,134:188].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-03T02:02:46.417952Z",
     "start_time": "2021-03-03T02:02:44.237756Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Weifa\\AppData\\Roaming\\Python\\Python37\\site-packages\\torchvision\\transforms\\functional.py:92: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  img = torch.from_numpy(np.array(pic, np.float32, copy=False))\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.parallel.data_parallel.DataParallel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'model.vdsr' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "E:\\Users\\Weifa\\Anaconda3\\envs\\py37\\lib\\site-packages\\torch\\serialization.py:658: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import datasets, models, transforms\n",
    "import numpy as np\n",
    "from model import vdsrd2\n",
    "from PIL import Image\n",
    "\n",
    "lr_transforms = transforms.Compose([\n",
    "    transforms.Resize((316, 376)),\n",
    "#     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.RandomRotation(30),\n",
    "    transforms.ToTensor()\n",
    "#     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "])\n",
    "\n",
    "lr=np.random.rand(79,94)\n",
    "lr_transforms(Image.fromarray(lr))\n",
    "\n",
    "result= torch.zeros((2,1,316,376),dtype=torch.float32)\n",
    "result[0]=lr_transforms(Image.fromarray(lr))\n",
    "# torch.tensor(sr).shape\n",
    "# self.lr_transform(Image.fromarray(lr))\n",
    "net_state=torch.load(\"../save/model/zg_val2012/best_test_20.pth\")['model'].module.state_dict()\n",
    "net = vdsrd2()\n",
    "net.load_state_dict(net_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T22:30:38.918041Z",
     "start_time": "2021-03-02T22:30:38.912057Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 316, 376])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare( l, device=False):\n",
    "    def _prepare(tensor):\n",
    "        tensor = tensor.float()\n",
    "        return tensor.to(device)\n",
    "    return [_prepare(_l) for _l in l]\n",
    "llrr=prepare([result],device)[0]\n",
    "llrr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T22:26:50.789567Z",
     "start_time": "2021-03-02T22:26:49.928389Z"
    }
   },
   "outputs": [],
   "source": [
    "result.to(device)\n",
    "with torch.set_grad_enabled(False):\n",
    "    aaaa=net(result.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-02T22:32:37.339491Z",
     "start_time": "2021-03-02T22:32:37.320569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 316, 376)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze (aaaa.cpu().numpy() ).shape\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
